{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:28:28.969186Z",
     "start_time": "2025-04-27T19:28:28.752505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "import src.common as common\n",
    "import src.ranking as ranking\n",
    "import src.data_loading as data_loading\n",
    "import src.data_transformation as data_transformation\n",
    "import src.user_profile as user_profile\n",
    "import src.test as test\n",
    "import traceback\n"
   ],
   "id": "69541cfbf080e9c4",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:28:37.032752Z",
     "start_time": "2025-04-27T19:28:28.982757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_loader = data_loading.DataLoader('../data')\n",
    "movies_df = data_loader.load_movies()\n",
    "ratings_df = data_loader.load_ratings()"
   ],
   "id": "d79e20a029fc8eb8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:28:42.832018Z",
     "start_time": "2025-04-27T19:28:37.302613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# split the data into train and test sets\n",
    "train_set, test_set, test_user_ids = test.RankingTest.split_ratings_into_train_test(ratings_df.copy())\n",
    "movie_rec_df = data_transformation.DataTransformer.transform_data(movies_df.copy(), train_set)\n",
    "results = []\n",
    "# i = 0\n",
    "# user_profiles = user_profile.UserProfileCreator.build_user_profiles(movie_rec_df, train_set)\n",
    "# user_profiles.head()\n",
    "# for user_id in sorted(test_user_ids):\n",
    "#     try:\n",
    "#         user_ratings = test_set[test_set['userId'] == user_id]\n",
    "#         train_user_ratings, test_user_ratings = test.RankingTest.split_user_ratings(user_ratings, n=0.1, k=10)\n",
    "#         test_user_profile = user_profile.UserProfileCreator.build_user_profile(movie_rec_df, train_user_ratings)\n",
    "#         ranking_df = ranking.RankingCreator.create_ranking(test_user_profile, movie_rec_df)\n",
    "#         # filter ranking_df so that it only includes movie_id from test user ratings\n",
    "#         test_movie_ids = test_user_ratings['movieId'].unique()\n",
    "#         filtered_ranking_df = ranking_df[ranking_df.index.isin(test_movie_ids)]\n",
    "#         results.append(test.RankingTest.calculate_spearman_corr(test_user_ratings.sort_values(by='rating', ascending=False), filtered_ranking_df))\n",
    "#         i+=1\n",
    "#         if i == 1000:\n",
    "#             break\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing user {user_id}: {e}\")\n",
    "#         print(traceback.format_exc())\n",
    "#         continue"
   ],
   "id": "f731c8bf25b32ec2",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:46:43.403694Z",
     "start_time": "2025-04-27T19:45:31.486623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.models import AprioriKmeansRecommender\n",
    "\n",
    "model = AprioriKmeansRecommender(movie_rec_df, min_support=0.1, k=21, min_confidence=0.1)\n",
    "model.fit(train_set)\n",
    "model.get_rules()\n"
   ],
   "id": "3a97d7df4cbddc17",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0:             antecedents          consequents  antecedent support  \\\n",
       " 0              (Action)          (Adventure)            0.497227   \n",
       " 1           (Adventure)             (Action)            0.342562   \n",
       " 2              (Action)             (Sci-Fi)            0.497227   \n",
       " 3              (Sci-Fi)             (Action)            0.550551   \n",
       " 4            (Thriller)             (Action)            0.380546   \n",
       " 5              (Action)           (Thriller)            0.497227   \n",
       " 6              (Sci-Fi)          (Adventure)            0.550551   \n",
       " 7           (Adventure)             (Sci-Fi)            0.342562   \n",
       " 8               (Drama)             (Sci-Fi)            0.330006   \n",
       " 9              (Sci-Fi)              (Drama)            0.550551   \n",
       " 10           (Thriller)              (Drama)            0.380546   \n",
       " 11              (Drama)           (Thriller)            0.330006   \n",
       " 12           (Thriller)             (Sci-Fi)            0.380546   \n",
       " 13             (Sci-Fi)           (Thriller)            0.550551   \n",
       " 14     (Action, Sci-Fi)          (Adventure)            0.337812   \n",
       " 15  (Action, Adventure)             (Sci-Fi)            0.236465   \n",
       " 16  (Sci-Fi, Adventure)             (Action)            0.234739   \n",
       " 17             (Action)  (Sci-Fi, Adventure)            0.497227   \n",
       " 18             (Sci-Fi)  (Action, Adventure)            0.550551   \n",
       " 19          (Adventure)     (Action, Sci-Fi)            0.342562   \n",
       " 20   (Thriller, Action)             (Sci-Fi)            0.219848   \n",
       " 21   (Thriller, Sci-Fi)             (Action)            0.230073   \n",
       " 22     (Action, Sci-Fi)           (Thriller)            0.337812   \n",
       " 23           (Thriller)     (Action, Sci-Fi)            0.380546   \n",
       " 24             (Action)   (Thriller, Sci-Fi)            0.497227   \n",
       " 25             (Sci-Fi)   (Thriller, Action)            0.550551   \n",
       " \n",
       "     consequent support   support  confidence      lift  representativity  \\\n",
       " 0             0.342562  0.236465    0.475568  1.388270               1.0   \n",
       " 1             0.497227  0.236465    0.690286  1.388270               1.0   \n",
       " 2             0.550551  0.337812    0.679391  1.234019               1.0   \n",
       " 3             0.497227  0.337812    0.613588  1.234019               1.0   \n",
       " 4             0.497227  0.219848    0.577718  1.161878               1.0   \n",
       " 5             0.380546  0.219848    0.442148  1.161878               1.0   \n",
       " 6             0.342562  0.234739    0.426370  1.244652               1.0   \n",
       " 7             0.550551  0.234739    0.685245  1.244652               1.0   \n",
       " 8             0.550551  0.124197    0.376347  0.683582               1.0   \n",
       " 9             0.330006  0.124197    0.225586  0.683582               1.0   \n",
       " 10            0.330006  0.119508    0.314043  0.951629               1.0   \n",
       " 11            0.380546  0.119508    0.362138  0.951629               1.0   \n",
       " 12            0.550551  0.230073    0.604587  1.098148               1.0   \n",
       " 13            0.380546  0.230073    0.417896  1.098148               1.0   \n",
       " 14            0.342562  0.173693    0.514171  1.500961               1.0   \n",
       " 15            0.550551  0.173693    0.734539  1.334188               1.0   \n",
       " 16            0.497227  0.173693    0.739943  1.488137               1.0   \n",
       " 17            0.234739  0.173693    0.349323  1.488137               1.0   \n",
       " 18            0.236465  0.173693    0.315489  1.334188               1.0   \n",
       " 19            0.337812  0.173693    0.507042  1.500961               1.0   \n",
       " 20            0.550551  0.152373    0.693082  1.258887               1.0   \n",
       " 21            0.497227  0.152373    0.662280  1.331946               1.0   \n",
       " 22            0.380546  0.152373    0.451058  1.185293               1.0   \n",
       " 23            0.337812  0.152373    0.400406  1.185293               1.0   \n",
       " 24            0.230073  0.152373    0.306445  1.331946               1.0   \n",
       " 25            0.219848  0.152373    0.276764  1.258887               1.0   \n",
       " \n",
       "     leverage  conviction  zhangs_metric   jaccard  certainty  kulczynski  \n",
       " 0   0.066134    1.253620       0.556274  0.391938   0.202310    0.582927  \n",
       " 1   0.066134    1.623345       0.425407  0.391938   0.383988    0.582927  \n",
       " 2   0.064062    1.401858       0.377188  0.475813   0.286661    0.646489  \n",
       " 3   0.064062    1.301131       0.421938  0.475813   0.231438    0.646489  \n",
       " 4   0.030630    1.190608       0.224915  0.334154   0.160093    0.509933  \n",
       " 5   0.030630    1.110427       0.277113  0.334154   0.099446    0.509933  \n",
       " 6   0.046141    1.146102       0.437342  0.356543   0.127477    0.555808  \n",
       " 7   0.046141    1.427932       0.298983  0.356543   0.299686    0.555808  \n",
       " 8  -0.057489    0.720671      -0.408590  0.164203  -0.387596    0.300967  \n",
       " 9  -0.057489    0.865163      -0.507362  0.164203  -0.155852    0.300967  \n",
       " 10 -0.006075    0.976729      -0.075834  0.202198  -0.023825    0.338091  \n",
       " 11 -0.006075    0.971142      -0.070517  0.202198  -0.029716    0.338091  \n",
       " 12  0.020563    1.136655       0.144281  0.328196   0.120226    0.511241  \n",
       " 13  0.020563    1.064163       0.198856  0.328196   0.060295    0.511241  \n",
       " 14  0.057972    1.353231       0.504026  0.342806   0.261028    0.510607  \n",
       " 15  0.043507    1.693088       0.328054  0.283200   0.409363    0.525014  \n",
       " 16  0.056975    1.933314       0.428637  0.311126   0.482753    0.544633  \n",
       " 17  0.056975    1.176101       0.652420  0.311126   0.149733    0.544633  \n",
       " 18  0.043507    1.115446       0.557306  0.283200   0.103497    0.525014  \n",
       " 19  0.057972    1.343296       0.507667  0.342806   0.255562    0.510607  \n",
       " 20  0.031335    1.464394       0.263599  0.246547   0.317123    0.484923  \n",
       " 21  0.037974    1.488727       0.323692  0.265030   0.328285    0.484363  \n",
       " 22  0.023820    1.128452       0.236076  0.269217   0.113830    0.425732  \n",
       " 23  0.023820    1.104394       0.252362  0.269217   0.094526    0.425732  \n",
       " 24  0.037974    1.110117       0.495689  0.265030   0.099194    0.484363  \n",
       " 25  0.031335    1.078696       0.457555  0.246547   0.072955    0.484923  ,\n",
       " 1:     antecedents  consequents  antecedent support  consequent support  \\\n",
       " 0      (Action)  (Adventure)            0.282563            0.208275   \n",
       " 1   (Adventure)     (Action)            0.208275            0.282563   \n",
       " 2      (Action)      (Drama)            0.282563            0.597197   \n",
       " 3       (Drama)     (Action)            0.597197            0.282563   \n",
       " 4      (Action)     (Sci-Fi)            0.282563            0.183941   \n",
       " 5      (Sci-Fi)     (Action)            0.183941            0.282563   \n",
       " 6    (Thriller)     (Action)            0.300103            0.282563   \n",
       " 7      (Action)   (Thriller)            0.282563            0.300103   \n",
       " 8       (Crime)      (Drama)            0.235517            0.597197   \n",
       " 9       (Drama)      (Crime)            0.597197            0.235517   \n",
       " 10   (Thriller)      (Crime)            0.300103            0.235517   \n",
       " 11      (Crime)   (Thriller)            0.235517            0.300103   \n",
       " 12   (Thriller)      (Drama)            0.300103            0.597197   \n",
       " 13      (Drama)   (Thriller)            0.597197            0.300103   \n",
       " \n",
       "      support  confidence      lift  representativity  leverage  conviction  \\\n",
       " 0   0.116861    0.413574  1.985712               1.0  0.058010    1.350086   \n",
       " 1   0.116861    0.561088  1.985712               1.0  0.058010    1.634583   \n",
       " 2   0.114231    0.404269  0.676944               1.0 -0.054514    0.676149   \n",
       " 3   0.114231    0.191279  0.676944               1.0 -0.054514    0.887126   \n",
       " 4   0.100765    0.356610  1.938717               1.0  0.048790    1.268373   \n",
       " 5   0.100765    0.547810  1.938717               1.0  0.048790    1.586582   \n",
       " 6   0.117835    0.392648  1.389596               1.0  0.033037    1.181255   \n",
       " 7   0.117835    0.417023  1.389596               1.0  0.033037    1.200555   \n",
       " 8   0.157069    0.666912  1.116738               1.0  0.016419    1.209300   \n",
       " 9   0.157069    0.263010  1.116738               1.0  0.016419    1.037305   \n",
       " 10  0.127231    0.423957  1.800116               1.0  0.056552    1.327130   \n",
       " 11  0.127231    0.540221  1.800116               1.0  0.056552    1.522246   \n",
       " 12  0.157089    0.523448  0.876509               1.0 -0.022132    0.845245   \n",
       " 13  0.157089    0.263043  0.876509               1.0 -0.022132    0.949712   \n",
       " \n",
       "     zhangs_metric   jaccard  certainty  kulczynski  \n",
       " 0        0.691910  0.312481   0.259306    0.487331  \n",
       " 1        0.626988  0.312481   0.388223    0.487331  \n",
       " 2       -0.399466  0.149219  -0.478963    0.297774  \n",
       " 3       -0.542285  0.149219  -0.127236    0.297774  \n",
       " 4        0.674895  0.275509   0.211588    0.452210  \n",
       " 5        0.593333  0.275509   0.369714    0.452210  \n",
       " 6        0.400583  0.253501   0.153443    0.404835  \n",
       " 7        0.390789  0.253501   0.167052    0.404835  \n",
       " 8        0.136739  0.232473   0.173075    0.464961  \n",
       " 9        0.259517  0.232473   0.035964    0.464961  \n",
       " 10       0.635066  0.311544   0.246494    0.482089  \n",
       " 11       0.581413  0.311544   0.343076    0.482089  \n",
       " 12      -0.167569  0.212221  -0.183088    0.393246  \n",
       " 13      -0.259135  0.212221  -0.052951    0.393246  ,\n",
       " 2:               antecedents            consequents  antecedent support  \\\n",
       " 0                (Action)            (Adventure)            0.616383   \n",
       " 1             (Adventure)               (Action)            0.346727   \n",
       " 2                (Action)                (Crime)            0.616383   \n",
       " 3                 (Crime)               (Action)            0.232954   \n",
       " 4                (Action)                (Drama)            0.616383   \n",
       " 5                 (Drama)               (Action)            0.318163   \n",
       " 6                (Action)               (Sci-Fi)            0.616383   \n",
       " 7                (Sci-Fi)               (Action)            0.267844   \n",
       " 8              (Thriller)               (Action)            0.443409   \n",
       " 9                (Action)             (Thriller)            0.616383   \n",
       " 10               (Sci-Fi)            (Adventure)            0.267844   \n",
       " 11            (Adventure)               (Sci-Fi)            0.346727   \n",
       " 12             (Thriller)            (Adventure)            0.443409   \n",
       " 13            (Adventure)             (Thriller)            0.346727   \n",
       " 14                (Crime)                (Drama)            0.232954   \n",
       " 15                (Drama)                (Crime)            0.318163   \n",
       " 16             (Thriller)                (Crime)            0.443409   \n",
       " 17                (Crime)             (Thriller)            0.232954   \n",
       " 18             (Thriller)                (Drama)            0.443409   \n",
       " 19                (Drama)             (Thriller)            0.318163   \n",
       " 20             (Thriller)               (Sci-Fi)            0.443409   \n",
       " 21               (Sci-Fi)             (Thriller)            0.267844   \n",
       " 22       (Action, Sci-Fi)            (Adventure)            0.211646   \n",
       " 23    (Action, Adventure)               (Sci-Fi)            0.274603   \n",
       " 24    (Sci-Fi, Adventure)               (Action)            0.136064   \n",
       " 25               (Action)    (Sci-Fi, Adventure)            0.616383   \n",
       " 26               (Sci-Fi)    (Action, Adventure)            0.267844   \n",
       " 27            (Adventure)       (Action, Sci-Fi)            0.346727   \n",
       " 28     (Thriller, Action)            (Adventure)            0.318716   \n",
       " 29  (Thriller, Adventure)               (Action)            0.124386   \n",
       " 30    (Action, Adventure)             (Thriller)            0.274603   \n",
       " 31             (Thriller)    (Action, Adventure)            0.443409   \n",
       " 32               (Action)  (Thriller, Adventure)            0.616383   \n",
       " 33            (Adventure)     (Thriller, Action)            0.346727   \n",
       " 34     (Thriller, Action)                (Crime)            0.318716   \n",
       " 35      (Thriller, Crime)               (Action)            0.153236   \n",
       " 36        (Action, Crime)             (Thriller)            0.145972   \n",
       " 37             (Thriller)        (Action, Crime)            0.443409   \n",
       " 38               (Action)      (Thriller, Crime)            0.616383   \n",
       " 39                (Crime)     (Thriller, Action)            0.232954   \n",
       " \n",
       "     consequent support   support  confidence      lift  representativity  \\\n",
       " 0             0.346727  0.274603    0.445507  1.284895               1.0   \n",
       " 1             0.616383  0.274603    0.791988  1.284895               1.0   \n",
       " 2             0.232954  0.145972    0.236821  1.016601               1.0   \n",
       " 3             0.616383  0.145972    0.626616  1.016601               1.0   \n",
       " 4             0.318163  0.139482    0.226291  0.711241               1.0   \n",
       " 5             0.616383  0.139482    0.438397  0.711241               1.0   \n",
       " 6             0.267844  0.211646    0.343368  1.281969               1.0   \n",
       " 7             0.616383  0.211646    0.790185  1.281969               1.0   \n",
       " 8             0.616383  0.318716    0.718786  1.166134               1.0   \n",
       " 9             0.443409  0.318716    0.517075  1.166134               1.0   \n",
       " 10            0.346727  0.136064    0.507996  1.465120               1.0   \n",
       " 11            0.267844  0.136064    0.392424  1.465120               1.0   \n",
       " 12            0.346727  0.124386    0.280522  0.809057               1.0   \n",
       " 13            0.443409  0.124386    0.358743  0.809057               1.0   \n",
       " 14            0.318163  0.104248    0.447507  1.406534               1.0   \n",
       " 15            0.232954  0.104248    0.327657  1.406534               1.0   \n",
       " 16            0.232954  0.153236    0.345586  1.483499               1.0   \n",
       " 17            0.443409  0.153236    0.657797  1.483499               1.0   \n",
       " 18            0.318163  0.122637    0.276578  0.869295               1.0   \n",
       " 19            0.443409  0.122637    0.385454  0.869295               1.0   \n",
       " 20            0.267844  0.118932    0.268222  1.001413               1.0   \n",
       " 21            0.443409  0.118932    0.444036  1.001413               1.0   \n",
       " 22            0.346727  0.118295    0.558927  1.612010               1.0   \n",
       " 23            0.267844  0.118295    0.430784  1.608339               1.0   \n",
       " 24            0.616383  0.118295    0.869406  1.410496               1.0   \n",
       " 25            0.136064  0.118295    0.191917  1.410496               1.0   \n",
       " 26            0.274603  0.118295    0.441655  1.608339               1.0   \n",
       " 27            0.211646  0.118295    0.341176  1.612010               1.0   \n",
       " 28            0.346727  0.120448    0.377916  1.089954               1.0   \n",
       " 29            0.616383  0.120448    0.968342  1.571005               1.0   \n",
       " 30            0.443409  0.120448    0.438625  0.989211               1.0   \n",
       " 31            0.274603  0.120448    0.271641  0.989211               1.0   \n",
       " 32            0.124386  0.120448    0.195411  1.571005               1.0   \n",
       " 33            0.318716  0.120448    0.347386  1.089954               1.0   \n",
       " 34            0.232954  0.102843    0.322678  1.385161               1.0   \n",
       " 35            0.616383  0.102843    0.671139  1.088834               1.0   \n",
       " 36            0.443409  0.102843    0.704536  1.588907               1.0   \n",
       " 37            0.145972  0.102843    0.231937  1.588907               1.0   \n",
       " 38            0.153236  0.102843    0.166849  1.088834               1.0   \n",
       " 39            0.318716  0.102843    0.441473  1.385161               1.0   \n",
       " \n",
       "     leverage  conviction  zhangs_metric   jaccard  certainty  kulczynski  \n",
       " 0   0.060887    1.178146       0.577989  0.398839   0.151209    0.618748  \n",
       " 1   0.060887    1.844203       0.339408  0.398839   0.457760    0.618748  \n",
       " 2   0.002384    1.005067       0.042567  0.207534   0.005042    0.431718  \n",
       " 3   0.002384    1.027404       0.021289  0.207534   0.026673    0.431718  \n",
       " 4  -0.056629    0.881257      -0.514169  0.175435  -0.134742    0.332344  \n",
       " 5  -0.056629    0.683075      -0.373213  0.175435  -0.463969    0.332344  \n",
       " 6   0.046552    1.115017       0.573359  0.314677   0.103153    0.566776  \n",
       " 7   0.046552    1.828353       0.300414  0.314677   0.453060    0.566776  \n",
       " 8   0.045406    1.364143       0.255961  0.430072   0.266939    0.617930  \n",
       " 9   0.045406    1.152540       0.371375  0.430072   0.132351    0.617930  \n",
       " 10  0.043195    1.327781       0.433599  0.284351   0.246864    0.450210  \n",
       " 11  0.043195    1.205044       0.485956  0.284351   0.170154    0.450210  \n",
       " 12 -0.029356    0.907982      -0.297764  0.186836  -0.101344    0.319632  \n",
       " 13 -0.029356    0.867969      -0.265391  0.186836  -0.152115    0.319632  \n",
       " 14  0.030131    1.234110       0.376812  0.233287   0.189700    0.387582  \n",
       " 15  0.030131    1.140856       0.423903  0.233287   0.123465    0.387582  \n",
       " 16  0.049942    1.172113       0.585561  0.292924   0.146840    0.501692  \n",
       " 17  0.049942    1.626493       0.424900  0.292924   0.385180    0.501692  \n",
       " 18 -0.018439    0.942516      -0.212685  0.191940  -0.060990    0.331016  \n",
       " 19 -0.018439    0.905694      -0.180675  0.191940  -0.104126    0.331016  \n",
       " 20  0.000168    1.000517       0.002534  0.200790   0.000517    0.356129  \n",
       " 21  0.000168    1.001127       0.001927  0.200790   0.001125    0.356129  \n",
       " 22  0.044911    1.481099       0.481581  0.268804   0.324826    0.450051  \n",
       " 23  0.044744    1.286253       0.521426  0.278897   0.222548    0.436220  \n",
       " 24  0.034427    2.937486       0.336865  0.186540   0.659573    0.530662  \n",
       " 25  0.034427    1.069119       0.758647  0.186540   0.064650    0.530662  \n",
       " 26  0.044744    1.299191       0.516612  0.278897   0.230290    0.436220  \n",
       " 27  0.044911    1.196607       0.581160  0.268804   0.164304    0.450051  \n",
       " 28  0.009941    1.050137       0.121139  0.221008   0.047743    0.362651  \n",
       " 29  0.043779   12.117347       0.415097  0.194170   0.917474    0.581876  \n",
       " 30 -0.001314    0.991478      -0.014813  0.201565  -0.008595    0.355133  \n",
       " 31 -0.001314    0.995932      -0.019219  0.201565  -0.004084    0.355133  \n",
       " 32  0.043779    1.088275       0.947469  0.194170   0.081114    0.581876  \n",
       " 33  0.009941    1.043931       0.126333  0.221008   0.042082    0.362651  \n",
       " 34  0.028597    1.132470       0.408145  0.229137   0.116974    0.382076  \n",
       " 35  0.008391    1.166501       0.096351  0.154239   0.142735    0.418994  \n",
       " 36  0.038117    1.883785       0.433987  0.211376   0.469154    0.468236  \n",
       " 37  0.038117    1.111923       0.665905  0.211376   0.100657    0.468236  \n",
       " 38  0.008391    1.016339       0.212676  0.154239   0.016076    0.418994  \n",
       " 39  0.028597    1.219787       0.362511  0.229137   0.180185    0.382076  ,\n",
       " 3:    antecedents  consequents  antecedent support  consequent support   support  \\\n",
       " 0     (Action)  (Adventure)            0.242358            0.259682  0.119350   \n",
       " 1  (Adventure)     (Action)            0.259682            0.242358  0.119350   \n",
       " 2      (Drama)     (Comedy)            0.501593            0.320618  0.102299   \n",
       " 3     (Comedy)      (Drama)            0.320618            0.501593  0.102299   \n",
       " 4      (Drama)    (Romance)            0.501593            0.190212  0.117228   \n",
       " 5    (Romance)      (Drama)            0.190212            0.501593  0.117228   \n",
       " \n",
       "    confidence      lift  representativity  leverage  conviction  \\\n",
       " 0    0.492452  1.896370               1.0  0.056414    1.458618   \n",
       " 1    0.459600  1.896370               1.0  0.056414    1.402002   \n",
       " 2    0.203948  0.636107               1.0 -0.058521    0.853439   \n",
       " 3    0.319067  0.636107               1.0 -0.058521    0.731947   \n",
       " 4    0.233710  1.228681               1.0  0.021818    1.056764   \n",
       " 5    0.616298  1.228681               1.0  0.021818    1.298943   \n",
       " \n",
       "    zhangs_metric   jaccard  certainty  kulczynski  \n",
       " 0       0.623878  0.311870   0.314420    0.476026  \n",
       " 1       0.638478  0.311870   0.286734    0.476026  \n",
       " 2      -0.534403  0.142099  -0.171731    0.261507  \n",
       " 3      -0.457122  0.142099  -0.366219    0.261507  \n",
       " 4       0.373428  0.204024   0.053715    0.425004  \n",
       " 5       0.229837  0.204024   0.230143    0.425004  ,\n",
       " 4:   antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0     (Drama)    (Comedy)            0.456105            0.455608  0.133670   \n",
       " 1    (Comedy)     (Drama)            0.455608            0.456105  0.133670   \n",
       " 2   (Romance)    (Comedy)            0.212667            0.455608  0.134477   \n",
       " 3    (Comedy)   (Romance)            0.455608            0.212667  0.134477   \n",
       " 4     (Drama)   (Romance)            0.456105            0.212667  0.116869   \n",
       " 5   (Romance)     (Drama)            0.212667            0.456105  0.116869   \n",
       " \n",
       "    confidence      lift  representativity  leverage  conviction  \\\n",
       " 0    0.293068  0.643247               1.0 -0.074135    0.770077   \n",
       " 1    0.293388  0.643247               1.0 -0.074135    0.769722   \n",
       " 2    0.632338  1.387899               1.0  0.037585    1.480685   \n",
       " 3    0.295160  1.387899               1.0  0.037585    1.117038   \n",
       " 4    0.256232  1.204852               1.0  0.019870    1.058574   \n",
       " 5    0.549539  1.204852               1.0  0.019870    1.207419   \n",
       " \n",
       "    zhangs_metric   jaccard  certainty  kulczynski  \n",
       " 0      -0.504879  0.171803  -0.298571    0.293228  \n",
       " 1      -0.504651  0.171803  -0.299170    0.293228  \n",
       " 2       0.354978  0.251926   0.324637    0.463749  \n",
       " 3       0.513392  0.251926   0.104775    0.463749  \n",
       " 4       0.312602  0.211756   0.055333    0.402886  \n",
       " 5       0.215948  0.211756   0.171787    0.402886  ,\n",
       " 5:   antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0     (Drama)    (Comedy)            0.621599            0.382002  0.164574   \n",
       " 1    (Comedy)     (Drama)            0.382002            0.621599  0.164574   \n",
       " 2   (Romance)    (Comedy)            0.271164            0.382002  0.143418   \n",
       " 3    (Comedy)   (Romance)            0.382002            0.271164  0.143418   \n",
       " 4     (Drama)   (Romance)            0.621599            0.271164  0.183129   \n",
       " 5   (Romance)     (Drama)            0.271164            0.621599  0.183129   \n",
       " \n",
       "    confidence      lift  representativity  leverage  conviction  \\\n",
       " 0    0.264760  0.693084               1.0 -0.072878    0.840539   \n",
       " 1    0.430821  0.693084               1.0 -0.072878    0.664818   \n",
       " 2    0.528896  1.384538               1.0  0.039832    1.311808   \n",
       " 3    0.375437  1.384538               1.0  0.039832    1.166953   \n",
       " 4    0.294610  1.086462               1.0  0.014574    1.033237   \n",
       " 5    0.675344  1.086462               1.0  0.014574    1.165543   \n",
       " \n",
       "    zhangs_metric   jaccard  certainty  kulczynski  \n",
       " 0      -0.539225  0.196149  -0.189713    0.347790  \n",
       " 1      -0.417436  0.196149  -0.504170    0.347790  \n",
       " 2       0.381070  0.281350   0.237693    0.452167  \n",
       " 3       0.449414  0.281350   0.143068    0.452167  \n",
       " 4       0.210309  0.258061   0.032168    0.484977  \n",
       " 5       0.109189  0.258061   0.142031    0.484977  ,\n",
       " 6:                antecedents             consequents  antecedent support  \\\n",
       " 0                 (Action)             (Adventure)            0.416082   \n",
       " 1              (Adventure)                (Action)            0.468690   \n",
       " 2                 (Action)                (Sci-Fi)            0.416082   \n",
       " 3                 (Sci-Fi)                (Action)            0.274813   \n",
       " 4               (Thriller)                (Action)            0.210480   \n",
       " 5                 (Action)              (Thriller)            0.416082   \n",
       " 6              (Animation)             (Adventure)            0.209004   \n",
       " 7              (Adventure)             (Animation)            0.468690   \n",
       " 8               (Children)             (Adventure)            0.193428   \n",
       " 9              (Adventure)              (Children)            0.468690   \n",
       " 10                (Comedy)             (Adventure)            0.323932   \n",
       " 11             (Adventure)                (Comedy)            0.468690   \n",
       " 12             (Adventure)               (Fantasy)            0.468690   \n",
       " 13               (Fantasy)             (Adventure)            0.245672   \n",
       " 14                (Sci-Fi)             (Adventure)            0.274813   \n",
       " 15             (Adventure)                (Sci-Fi)            0.468690   \n",
       " 16             (Animation)              (Children)            0.209004   \n",
       " 17              (Children)             (Animation)            0.193428   \n",
       " 18             (Animation)                (Comedy)            0.209004   \n",
       " 19                (Comedy)             (Animation)            0.323932   \n",
       " 20              (Children)                (Comedy)            0.193428   \n",
       " 21                (Comedy)              (Children)            0.323932   \n",
       " 22        (Action, Sci-Fi)             (Adventure)            0.196276   \n",
       " 23     (Action, Adventure)                (Sci-Fi)            0.248699   \n",
       " 24     (Sci-Fi, Adventure)                (Action)            0.164962   \n",
       " 25                (Action)     (Sci-Fi, Adventure)            0.416082   \n",
       " 26                (Sci-Fi)     (Action, Adventure)            0.274813   \n",
       " 27             (Adventure)        (Action, Sci-Fi)            0.468690   \n",
       " 28  (Animation, Adventure)              (Children)            0.142185   \n",
       " 29   (Animation, Children)             (Adventure)            0.156100   \n",
       " 30   (Children, Adventure)             (Animation)            0.136713   \n",
       " 31             (Animation)   (Children, Adventure)            0.209004   \n",
       " 32             (Adventure)   (Animation, Children)            0.468690   \n",
       " 33              (Children)  (Animation, Adventure)            0.193428   \n",
       " \n",
       "     consequent support   support  confidence      lift  representativity  \\\n",
       " 0             0.468690  0.248699    0.597715  1.275287               1.0   \n",
       " 1             0.416082  0.248699    0.530624  1.275287               1.0   \n",
       " 2             0.274813  0.196276    0.471724  1.716531               1.0   \n",
       " 3             0.416082  0.196276    0.714218  1.716531               1.0   \n",
       " 4             0.416082  0.130255    0.618849  1.487324               1.0   \n",
       " 5             0.210480  0.130255    0.313051  1.487324               1.0   \n",
       " 6             0.468690  0.142185    0.680294  1.451479               1.0   \n",
       " 7             0.209004  0.142185    0.303366  1.451479               1.0   \n",
       " 8             0.468690  0.136713    0.706790  1.508010               1.0   \n",
       " 9             0.193428  0.136713    0.291692  1.508010               1.0   \n",
       " 10            0.468690  0.139570    0.430861  0.919288               1.0   \n",
       " 11            0.323932  0.139570    0.297787  0.919288               1.0   \n",
       " 12            0.245672  0.187009    0.399004  1.624137               1.0   \n",
       " 13            0.468690  0.187009    0.761217  1.624137               1.0   \n",
       " 14            0.468690  0.164962    0.600269  1.280737               1.0   \n",
       " 15            0.274813  0.164962    0.351963  1.280737               1.0   \n",
       " 16            0.193428  0.156100    0.746874  3.861252               1.0   \n",
       " 17            0.209004  0.156100    0.807019  3.861252               1.0   \n",
       " 18            0.323932  0.112609    0.538788  1.663274               1.0   \n",
       " 19            0.209004  0.112609    0.347632  1.663274               1.0   \n",
       " 20            0.323932  0.109292    0.565029  1.744282               1.0   \n",
       " 21            0.193428  0.109292    0.337393  1.744282               1.0   \n",
       " 22            0.468690  0.129957    0.662113  1.412688               1.0   \n",
       " 23            0.274813  0.129957    0.522549  1.901472               1.0   \n",
       " 24            0.416082  0.129957    0.787802  1.893381               1.0   \n",
       " 25            0.164962  0.129957    0.312335  1.893381               1.0   \n",
       " 26            0.248699  0.129957    0.472893  1.901472               1.0   \n",
       " 27            0.196276  0.129957    0.277277  1.412688               1.0   \n",
       " 28            0.193428  0.114502    0.805303  4.163319               1.0   \n",
       " 29            0.468690  0.114502    0.733514  1.565028               1.0   \n",
       " 30            0.209004  0.114502    0.837532  4.007246               1.0   \n",
       " 31            0.136713  0.114502    0.547843  4.007246               1.0   \n",
       " 32            0.156100  0.114502    0.244301  1.565028               1.0   \n",
       " 33            0.142185  0.114502    0.591959  4.163319               1.0   \n",
       " \n",
       "     leverage  conviction  zhangs_metric   jaccard  certainty  kulczynski  \n",
       " 0   0.053685    1.320728       0.369680  0.390990   0.242842    0.564170  \n",
       " 1   0.053685    1.244031       0.406284  0.390990   0.196161    0.564170  \n",
       " 2   0.081932    1.372745       0.714878  0.396823   0.271532    0.592971  \n",
       " 3   0.081932    2.043230       0.575616  0.396823   0.510579    0.592971  \n",
       " 4   0.042678    1.531986       0.415001  0.262449   0.347252    0.465950  \n",
       " 5   0.042678    1.149315       0.561126  0.262449   0.129917    0.465950  \n",
       " 6   0.044226    1.661870       0.393235  0.265512   0.398268    0.491830  \n",
       " 7   0.044226    1.135453       0.585435  0.265512   0.119294    0.491830  \n",
       " 8   0.046055    1.812044       0.417662  0.260205   0.448137    0.499241  \n",
       " 9   0.046055    1.138730       0.634046  0.260205   0.121829    0.499241  \n",
       " 10 -0.012254    0.933533      -0.114940  0.213719  -0.071200    0.364324  \n",
       " 11 -0.012254    0.962767      -0.141815  0.213719  -0.038673    0.364324  \n",
       " 12  0.071866    1.255131       0.723285  0.354619   0.203270    0.580111  \n",
       " 13  0.071866    2.225076       0.509444  0.354619   0.550577    0.580111  \n",
       " 14  0.036159    1.329168       0.302266  0.285133   0.247650    0.476116  \n",
       " 15  0.036159    1.119052       0.412564  0.285133   0.106386    0.476116  \n",
       " 16  0.115673    3.186449       0.936815  0.633697   0.686171    0.776947  \n",
       " 17  0.115673    4.098824       0.918724  0.633697   0.756028    0.776947  \n",
       " 18  0.044906    1.465850       0.504145  0.267908   0.317802    0.443210  \n",
       " 19  0.044906    1.212498       0.589846  0.267908   0.175257    0.443210  \n",
       " 20  0.046635    1.554282       0.529027  0.267829   0.356616    0.451211  \n",
       " 21  0.046635    1.217271       0.631147  0.267829   0.178490    0.451211  \n",
       " 22  0.037964    1.572448       0.363470  0.242906   0.364049    0.469695  \n",
       " 23  0.061612    1.518872       0.631027  0.330214   0.341616    0.497721  \n",
       " 24  0.061319    2.751764       0.565057  0.288098   0.636597    0.550069  \n",
       " 25  0.061319    1.214310       0.808066  0.288098   0.176487    0.550069  \n",
       " 26  0.061612    1.425331       0.653751  0.330214   0.298409    0.497721  \n",
       " 27  0.037964    1.112077       0.549829  0.242906   0.100782    0.469695  \n",
       " 28  0.086999    4.142696       0.885746  0.517847   0.758611    0.698631  \n",
       " 29  0.041339    1.993760       0.427816  0.224386   0.498435    0.488907  \n",
       " 30  0.085928    4.868635       0.869296  0.495215   0.794604    0.692687  \n",
       " 31  0.085928    1.909262       0.948744  0.495215   0.476237    0.692687  \n",
       " 32  0.041339    1.116714       0.679517  0.224386   0.104516    0.488907  \n",
       " 33  0.086999    2.102280       0.942020  0.517847   0.524326    0.698631  ,\n",
       " 7:           antecedents        consequents  antecedent support  \\\n",
       " 0             (Drama)           (Comedy)            0.568173   \n",
       " 1            (Comedy)            (Drama)            0.537315   \n",
       " 2           (Romance)           (Comedy)            0.534932   \n",
       " 3            (Comedy)          (Romance)            0.537315   \n",
       " 4             (Drama)          (Romance)            0.568173   \n",
       " 5           (Romance)            (Drama)            0.534932   \n",
       " 6    (Drama, Romance)           (Comedy)            0.311362   \n",
       " 7     (Drama, Comedy)          (Romance)            0.207137   \n",
       " 8   (Romance, Comedy)            (Drama)            0.331851   \n",
       " 9             (Drama)  (Romance, Comedy)            0.568173   \n",
       " 10          (Romance)    (Drama, Comedy)            0.534932   \n",
       " 11           (Comedy)   (Drama, Romance)            0.537315   \n",
       " \n",
       "     consequent support   support  confidence      lift  representativity  \\\n",
       " 0             0.537315  0.207137    0.364567  0.678498               1.0   \n",
       " 1             0.568173  0.207137    0.385504  0.678498               1.0   \n",
       " 2             0.537315  0.331851    0.620362  1.154559               1.0   \n",
       " 3             0.534932  0.331851    0.617610  1.154559               1.0   \n",
       " 4             0.534932  0.311362    0.548005  1.024440               1.0   \n",
       " 5             0.568173  0.311362    0.582059  1.024440               1.0   \n",
       " 6             0.537315  0.134412    0.431693  0.803425               1.0   \n",
       " 7             0.534932  0.134412    0.648905  1.213062               1.0   \n",
       " 8             0.568173  0.134412    0.405038  0.712879               1.0   \n",
       " 9             0.331851  0.134412    0.236570  0.712879               1.0   \n",
       " 10            0.207137  0.134412    0.251270  1.213062               1.0   \n",
       " 11            0.311362  0.134412    0.250156  0.803425               1.0   \n",
       " \n",
       "     leverage  conviction  zhangs_metric   jaccard  certainty  kulczynski  \n",
       " 0  -0.098151    0.728142      -0.523196  0.230575  -0.373359    0.375036  \n",
       " 1  -0.098151    0.702734      -0.505957  0.230575  -0.423013    0.375036  \n",
       " 2   0.044424    1.218753       0.287847  0.448208   0.179489    0.618986  \n",
       " 3   0.044424    1.216215       0.289330  0.448208   0.177777    0.618986  \n",
       " 4   0.007428    1.028924       0.055246  0.393261   0.028111    0.565032  \n",
       " 5   0.007428    1.033225       0.051297  0.393261   0.032156    0.565032  \n",
       " 6  -0.032887    0.814145      -0.262154  0.188183  -0.228282    0.340924  \n",
       " 7   0.023608    1.324624       0.221526  0.221198   0.245069    0.450088  \n",
       " 8  -0.054136    0.725807      -0.376093  0.175562  -0.377777    0.320804  \n",
       " 9  -0.054136    0.875193      -0.482588  0.175562  -0.142605    0.320804  \n",
       " 10  0.023608    1.058944       0.377665  0.221198   0.055663    0.450088  \n",
       " 11 -0.032887    0.918375      -0.345895  0.188183  -0.088879    0.340924  ,\n",
       " 8:             antecedents          consequents  antecedent support  \\\n",
       " 0              (Action)          (Adventure)            0.422066   \n",
       " 1           (Adventure)             (Action)            0.325729   \n",
       " 2              (Action)              (Drama)            0.422066   \n",
       " 3               (Drama)             (Action)            0.423063   \n",
       " 4              (Action)             (Sci-Fi)            0.422066   \n",
       " 5              (Sci-Fi)             (Action)            0.278008   \n",
       " 6            (Thriller)             (Action)            0.317130   \n",
       " 7              (Action)           (Thriller)            0.422066   \n",
       " 8              (Sci-Fi)          (Adventure)            0.278008   \n",
       " 9           (Adventure)             (Sci-Fi)            0.325729   \n",
       " 10              (Crime)              (Drama)            0.202464   \n",
       " 11              (Drama)              (Crime)            0.423063   \n",
       " 12           (Thriller)              (Crime)            0.317130   \n",
       " 13              (Crime)           (Thriller)            0.202464   \n",
       " 14           (Thriller)              (Drama)            0.317130   \n",
       " 15              (Drama)           (Thriller)            0.423063   \n",
       " 16           (Thriller)             (Sci-Fi)            0.317130   \n",
       " 17             (Sci-Fi)           (Thriller)            0.278008   \n",
       " 18     (Action, Sci-Fi)          (Adventure)            0.185398   \n",
       " 19  (Action, Adventure)             (Sci-Fi)            0.203377   \n",
       " 20  (Sci-Fi, Adventure)             (Action)            0.134025   \n",
       " 21             (Action)  (Sci-Fi, Adventure)            0.422066   \n",
       " 22             (Sci-Fi)  (Action, Adventure)            0.278008   \n",
       " 23          (Adventure)     (Action, Sci-Fi)            0.325729   \n",
       " \n",
       "     consequent support   support  confidence      lift  representativity  \\\n",
       " 0             0.325729  0.203377    0.481861  1.479333               1.0   \n",
       " 1             0.422066  0.203377    0.624375  1.479333               1.0   \n",
       " 2             0.423063  0.113696    0.269379  0.636735               1.0   \n",
       " 3             0.422066  0.113696    0.268744  0.636735               1.0   \n",
       " 4             0.278008  0.185398    0.439263  1.580041               1.0   \n",
       " 5             0.422066  0.185398    0.666881  1.580041               1.0   \n",
       " 6             0.422066  0.169745    0.535253  1.268175               1.0   \n",
       " 7             0.317130  0.169745    0.402176  1.268175               1.0   \n",
       " 8             0.325729  0.134025    0.482090  1.480035               1.0   \n",
       " 9             0.278008  0.134025    0.411461  1.480035               1.0   \n",
       " 10            0.423063  0.111035    0.548418  1.296305               1.0   \n",
       " 11            0.202464  0.111035    0.262454  1.296305               1.0   \n",
       " 12            0.202464  0.114167    0.360000  1.778096               1.0   \n",
       " 13            0.317130  0.114167    0.563887  1.778096               1.0   \n",
       " 14            0.423063  0.125999    0.397310  0.939129               1.0   \n",
       " 15            0.317130  0.125999    0.297826  0.939129               1.0   \n",
       " 16            0.278008  0.111239    0.350768  1.261722               1.0   \n",
       " 17            0.317130  0.111239    0.400130  1.261722               1.0   \n",
       " 18            0.325729  0.106928    0.576748  1.770638               1.0   \n",
       " 19            0.278008  0.106928    0.525762  1.891177               1.0   \n",
       " 20            0.422066  0.106928    0.797822  1.890280               1.0   \n",
       " 21            0.134025  0.106928    0.253344  1.890280               1.0   \n",
       " 22            0.203377  0.106928    0.384622  1.891177               1.0   \n",
       " 23            0.185398  0.106928    0.328273  1.770638               1.0   \n",
       " \n",
       "     leverage  conviction  zhangs_metric   jaccard  certainty  kulczynski  \n",
       " 0   0.065898    1.301333       0.560651  0.373568   0.231557    0.553118  \n",
       " 1   0.065898    1.538596       0.480548  0.373568   0.350057    0.553118  \n",
       " 2  -0.064865    0.789653      -0.496768  0.155442  -0.266378    0.269062  \n",
       " 3  -0.064865    0.790331      -0.497200  0.155442  -0.265292    0.269062  \n",
       " 4   0.068061    1.287579       0.635202  0.360223   0.223348    0.553072  \n",
       " 5   0.068061    1.734918       0.508461  0.360223   0.423604    0.553072  \n",
       " 6   0.035895    1.243547       0.309671  0.298085   0.195848    0.468715  \n",
       " 7   0.035895    1.142260       0.365899  0.298085   0.124542    0.468715  \n",
       " 8   0.043470    1.301908       0.449230  0.285334   0.231897    0.446775  \n",
       " 9   0.043470    1.226754       0.481024  0.285334   0.184840    0.446775  \n",
       " 10  0.025380    1.277592       0.286603  0.215814   0.217277    0.405436  \n",
       " 11  0.025380    1.081339       0.396189  0.215814   0.075220    0.405436  \n",
       " 12  0.049959    1.246150       0.640826  0.281596   0.197528    0.461943  \n",
       " 13  0.049959    1.565811       0.548691  0.281596   0.361353    0.461943  \n",
       " 14 -0.008167    0.957271      -0.086689  0.205145  -0.044636    0.347568  \n",
       " 15 -0.008167    0.972508      -0.100999  0.205145  -0.028269    0.347568  \n",
       " 16  0.023075    1.112072       0.303766  0.229881   0.100778    0.375449  \n",
       " 17  0.023075    1.138363       0.287306  0.229881   0.121546    0.375449  \n",
       " 18  0.046538    1.593071       0.534288  0.264543   0.372282    0.452510  \n",
       " 19  0.050387    1.522425       0.591533  0.285554   0.343153    0.455192  \n",
       " 20  0.050361    2.858544       0.543870  0.238060   0.650172    0.525583  \n",
       " 21  0.050361    1.159805       0.814933  0.238060   0.137786    0.525583  \n",
       " 22  0.050387    1.294526       0.652678  0.285554   0.227517    0.455192  \n",
       " 23  0.046538    1.212698       0.645485  0.264543   0.175392    0.452510  ,\n",
       " 9:   antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0     (Drama)    (Comedy)            0.574701            0.307293  0.121647   \n",
       " 1    (Comedy)     (Drama)            0.307293            0.574701  0.121647   \n",
       " 2     (Crime)     (Drama)            0.208860            0.574701  0.126891   \n",
       " 3     (Drama)     (Crime)            0.574701            0.208860  0.126891   \n",
       " 4  (Thriller)     (Crime)            0.267792            0.208860  0.113571   \n",
       " 5     (Crime)  (Thriller)            0.208860            0.267792  0.113571   \n",
       " 6     (Drama)   (Romance)            0.574701            0.185780  0.124773   \n",
       " 7   (Romance)     (Drama)            0.185780            0.574701  0.124773   \n",
       " 8  (Thriller)     (Drama)            0.267792            0.574701  0.130304   \n",
       " 9     (Drama)  (Thriller)            0.574701            0.267792  0.130304   \n",
       " \n",
       "    confidence      lift  representativity  leverage  conviction  \\\n",
       " 0    0.211671  0.688823               1.0 -0.054954    0.878702   \n",
       " 1    0.395868  0.688823               1.0 -0.054954    0.703983   \n",
       " 2    0.607538  1.057137               1.0  0.006858    1.083668   \n",
       " 3    0.220794  1.057137               1.0  0.006858    1.015315   \n",
       " 4    0.424104  2.030561               1.0  0.057640    1.373754   \n",
       " 5    0.543767  2.030561               1.0  0.057640    1.604901   \n",
       " 6    0.217109  1.168636               1.0  0.018005    1.040017   \n",
       " 7    0.671617  1.168636               1.0  0.018005    1.295129   \n",
       " 8    0.486589  0.846681               1.0 -0.023596    0.828378   \n",
       " 9    0.226734  0.846681               1.0 -0.023596    0.946904   \n",
       " \n",
       "    zhangs_metric   jaccard  certainty  kulczynski  \n",
       " 0      -0.515080  0.159989  -0.138042    0.303769  \n",
       " 1      -0.394729  0.159989  -0.420489    0.303769  \n",
       " 2       0.068318  0.193233   0.077208    0.414166  \n",
       " 3       0.127084  0.193233   0.015084    0.414166  \n",
       " 4       0.693143  0.312800   0.272068    0.483936  \n",
       " 5       0.641512  0.312800   0.376908    0.483936  \n",
       " 6       0.339295  0.196274   0.038478    0.444363  \n",
       " 7       0.177227  0.196274   0.227876    0.444363  \n",
       " 8      -0.198275  0.182963  -0.207178    0.356661  \n",
       " 9      -0.298628  0.182963  -0.056074    0.356661  ,\n",
       " 10:   antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0     (Drama)    (Comedy)            0.792402            0.269121  0.166670   \n",
       " 1    (Comedy)     (Drama)            0.269121            0.792402  0.166670   \n",
       " 2   (Romance)    (Comedy)            0.262683            0.269121  0.102119   \n",
       " 3    (Comedy)   (Romance)            0.269121            0.262683  0.102119   \n",
       " 4     (Crime)     (Drama)            0.140920            0.792402  0.105880   \n",
       " 5     (Drama)     (Crime)            0.792402            0.140920  0.105880   \n",
       " 6     (Drama)   (Romance)            0.792402            0.262683  0.216011   \n",
       " 7   (Romance)     (Drama)            0.262683            0.792402  0.216011   \n",
       " \n",
       "    confidence      lift  representativity  leverage  conviction  \\\n",
       " 0    0.210335  0.781561               1.0 -0.046583    0.925555   \n",
       " 1    0.619310  0.781561               1.0 -0.046583    0.545321   \n",
       " 2    0.388755  1.444533               1.0  0.031426    1.195721   \n",
       " 3    0.379454  1.444533               1.0  0.031426    1.188175   \n",
       " 4    0.751346  0.948189               1.0 -0.005786    0.834889   \n",
       " 5    0.133619  0.948189               1.0 -0.005786    0.991573   \n",
       " 6    0.272603  1.037763               1.0  0.007860    1.013637   \n",
       " 7    0.822325  1.037763               1.0  0.007860    1.168418   \n",
       " \n",
       "    zhangs_metric   jaccard  certainty  kulczynski  \n",
       " 0      -0.573798  0.186253  -0.080433    0.414822  \n",
       " 1      -0.276623  0.186253  -0.833782    0.414822  \n",
       " 2       0.417371  0.237661   0.163684    0.384104  \n",
       " 3       0.421048  0.237661   0.158373    0.384104  \n",
       " 4      -0.059802  0.127960  -0.197764    0.442483  \n",
       " 5      -0.208367  0.127960  -0.008499    0.442483  \n",
       " 6       0.175286  0.257439   0.013454    0.547464  \n",
       " 7       0.049353  0.257439   0.144142    0.547464  ,\n",
       " 11:               antecedents                       consequents  \\\n",
       " 0                (Action)                       (Adventure)   \n",
       " 1             (Adventure)                          (Action)   \n",
       " 2             (Animation)                       (Adventure)   \n",
       " 3             (Adventure)                       (Animation)   \n",
       " 4              (Children)                       (Adventure)   \n",
       " ..                    ...                               ...   \n",
       " 63  (Children, Adventure)               (Animation, Comedy)   \n",
       " 64            (Animation)     (Adventure, Children, Comedy)   \n",
       " 65               (Comedy)  (Animation, Adventure, Children)   \n",
       " 66            (Adventure)     (Animation, Comedy, Children)   \n",
       " 67             (Children)    (Adventure, Animation, Comedy)   \n",
       " \n",
       "     antecedent support  consequent support   support  confidence      lift  \\\n",
       " 0             0.229821            0.430554  0.144274    0.627767  1.458043   \n",
       " 1             0.430554            0.229821  0.144274    0.335088  1.458043   \n",
       " 2             0.362581            0.430554  0.217259    0.599201  1.391696   \n",
       " 3             0.430554            0.362581  0.217259    0.504603  1.391696   \n",
       " 4             0.376209            0.430554  0.223894    0.595133  1.382249   \n",
       " ..                 ...                 ...       ...         ...       ...   \n",
       " 63            0.223894            0.185670  0.114382    0.510873  2.751510   \n",
       " 64            0.362581            0.127061  0.114382    0.315465  2.482787   \n",
       " 65            0.442198            0.182933  0.114382    0.258666  1.413993   \n",
       " 66            0.430554            0.156191  0.114382    0.265661  1.700871   \n",
       " 67            0.376209            0.126021  0.114382    0.304038  2.412597   \n",
       " \n",
       "     representativity  leverage  conviction  zhangs_metric   jaccard  \\\n",
       " 0                1.0  0.045323    1.529808       0.407891  0.279545   \n",
       " 1                1.0  0.045323    1.158318       0.551675  0.279545   \n",
       " 2                1.0  0.061148    1.420775       0.441550  0.377267   \n",
       " 3                1.0  0.061148    1.286683       0.494257  0.377267   \n",
       " 4                1.0  0.061916    1.406501       0.443324  0.384125   \n",
       " ..               ...       ...         ...            ...       ...   \n",
       " 63               1.0  0.072811    1.664864       0.820202  0.387494   \n",
       " 64               1.0  0.068312    1.275229       0.936946  0.304806   \n",
       " 65               1.0  0.033489    1.102158       0.524887  0.223949   \n",
       " 66               1.0  0.047133    1.149073       0.723627  0.242147   \n",
       " 67               1.0  0.066971    1.255785       0.938630  0.294914   \n",
       " \n",
       "     certainty  kulczynski  \n",
       " 0    0.346323    0.481427  \n",
       " 1    0.136679    0.481427  \n",
       " 2    0.296159    0.551902  \n",
       " 3    0.222808    0.551902  \n",
       " 4    0.289016    0.557574  \n",
       " ..        ...         ...  \n",
       " 63   0.399351    0.563461  \n",
       " 64   0.215827    0.607839  \n",
       " 65   0.092689    0.441965  \n",
       " 66   0.129733    0.498989  \n",
       " 67   0.203685    0.605839  \n",
       " \n",
       " [68 rows x 14 columns],\n",
       " 12:   antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0     (Drama)    (Comedy)            0.704303            0.232909  0.124068   \n",
       " 1    (Comedy)     (Drama)            0.232909            0.704303  0.124068   \n",
       " 2     (Crime)     (Drama)            0.207217            0.704303  0.144416   \n",
       " 3     (Drama)     (Crime)            0.704303            0.207217  0.144416   \n",
       " 4  (Thriller)     (Crime)            0.227373            0.207217  0.103355   \n",
       " 5     (Crime)  (Thriller)            0.207217            0.227373  0.103355   \n",
       " 6     (Drama)   (Romance)            0.704303            0.187645  0.146476   \n",
       " 7   (Romance)     (Drama)            0.187645            0.704303  0.146476   \n",
       " 8  (Thriller)     (Drama)            0.227373            0.704303  0.133978   \n",
       " 9     (Drama)  (Thriller)            0.704303            0.227373  0.133978   \n",
       " \n",
       "    confidence      lift  representativity  leverage  conviction  \\\n",
       " 0    0.176157  0.756335               1.0 -0.039970    0.931113   \n",
       " 1    0.532689  0.756335               1.0 -0.039970    0.632763   \n",
       " 2    0.696928  0.989529               1.0 -0.001528    0.975666   \n",
       " 3    0.205048  0.989529               1.0 -0.001528    0.997271   \n",
       " 4    0.454560  2.193641               1.0  0.056239    1.453475   \n",
       " 5    0.498774  2.193641               1.0  0.056239    1.541474   \n",
       " 6    0.207973  1.108329               1.0  0.014317    1.025665   \n",
       " 7    0.780599  1.108329               1.0  0.014317    1.347749   \n",
       " 8    0.589245  0.836636               1.0 -0.026161    0.719886   \n",
       " 9    0.190228  0.836636               1.0 -0.026161    0.954130   \n",
       " \n",
       "    zhangs_metric   jaccard  certainty  kulczynski  \n",
       " 0      -0.521419  0.152578  -0.073983    0.354423  \n",
       " 1      -0.295766  0.152578  -0.580371    0.354423  \n",
       " 2      -0.013172  0.188261  -0.024941    0.450988  \n",
       " 3      -0.034550  0.188261  -0.002737    0.450988  \n",
       " 4       0.704268  0.312028   0.311994    0.476667  \n",
       " 5       0.686363  0.312028   0.351270    0.476667  \n",
       " 6       0.330543  0.196487   0.025023    0.494286  \n",
       " 7       0.120318  0.196487   0.258022    0.494286  \n",
       " 8      -0.201741  0.167956  -0.389108    0.389737  \n",
       " 9      -0.397717  0.167956  -0.048076    0.389737  ,\n",
       " 13:           antecedents        consequents  antecedent support  \\\n",
       " 0            (Action)            (Drama)            0.226490   \n",
       " 1             (Drama)           (Action)            0.648466   \n",
       " 2          (Thriller)           (Action)            0.408462   \n",
       " 3            (Action)         (Thriller)            0.226490   \n",
       " 4             (Crime)            (Drama)            0.370879   \n",
       " 5             (Drama)            (Crime)            0.648466   \n",
       " 6          (Thriller)            (Crime)            0.408462   \n",
       " 7             (Crime)         (Thriller)            0.370879   \n",
       " 8          (Thriller)            (Drama)            0.408462   \n",
       " 9             (Drama)         (Thriller)            0.648466   \n",
       " 10         (Thriller)          (Mystery)            0.408462   \n",
       " 11          (Mystery)         (Thriller)            0.166157   \n",
       " 12  (Thriller, Crime)            (Drama)            0.215085   \n",
       " 13  (Thriller, Drama)            (Crime)            0.227187   \n",
       " 14     (Crime, Drama)         (Thriller)            0.253679   \n",
       " 15         (Thriller)     (Crime, Drama)            0.408462   \n",
       " 16            (Crime)  (Thriller, Drama)            0.370879   \n",
       " 17            (Drama)  (Thriller, Crime)            0.648466   \n",
       " \n",
       "     consequent support   support  confidence      lift  representativity  \\\n",
       " 0             0.648466  0.113704    0.502025  0.774173               1.0   \n",
       " 1             0.226490  0.113704    0.175343  0.774173               1.0   \n",
       " 2             0.226490  0.122190    0.299147  1.320793               1.0   \n",
       " 3             0.408462  0.122190    0.539494  1.320793               1.0   \n",
       " 4             0.648466  0.253679    0.683995  1.054789               1.0   \n",
       " 5             0.370879  0.253679    0.391199  1.054789               1.0   \n",
       " 6             0.370879  0.215085    0.526574  1.419800               1.0   \n",
       " 7             0.408462  0.215085    0.579934  1.419800               1.0   \n",
       " 8             0.648466  0.227187    0.556200  0.857716               1.0   \n",
       " 9             0.408462  0.227187    0.350345  0.857716               1.0   \n",
       " 10            0.166157  0.135772    0.332398  2.000508               1.0   \n",
       " 11            0.408462  0.135772    0.817132  2.000508               1.0   \n",
       " 12            0.648466  0.128123    0.595686  0.918607               1.0   \n",
       " 13            0.370879  0.128123    0.563956  1.520594               1.0   \n",
       " 14            0.408462  0.128123    0.505060  1.236493               1.0   \n",
       " 15            0.253679  0.128123    0.313672  1.236493               1.0   \n",
       " 16            0.227187  0.128123    0.345459  1.520594               1.0   \n",
       " 17            0.215085  0.128123    0.197579  0.918607               1.0   \n",
       " \n",
       "     leverage  conviction  zhangs_metric   jaccard  certainty  kulczynski  \n",
       " 0  -0.033168    0.705926      -0.273844  0.149364  -0.416579    0.338684  \n",
       " 1  -0.033168    0.937977      -0.453491  0.149364  -0.066124    0.338684  \n",
       " 2   0.029677    1.103669       0.410590  0.238298   0.093931    0.419320  \n",
       " 3   0.029677    1.284539       0.313997  0.238298   0.221511    0.419320  \n",
       " 4   0.013177    1.112431       0.082564  0.331318   0.101067    0.537597  \n",
       " 5   0.013177    1.033377       0.147761  0.331318   0.032299    0.537597  \n",
       " 6   0.063595    1.328868       0.499842  0.381184   0.247480    0.553254  \n",
       " 7   0.063595    1.408203       0.469981  0.381184   0.289875    0.553254  \n",
       " 8  -0.037687    0.792100      -0.219014  0.273804  -0.262467    0.453272  \n",
       " 9  -0.037687    0.910541      -0.320603  0.273804  -0.098248    0.453272  \n",
       " 10  0.067903    1.249012       0.845469  0.309383   0.199367    0.574765  \n",
       " 11  0.067903    3.234777       0.599785  0.309383   0.690860    0.574765  \n",
       " 12 -0.011352    0.869457      -0.101434  0.174216  -0.150143    0.396632  \n",
       " 13  0.043865    1.442794       0.443008  0.272636   0.306900    0.454707  \n",
       " 14  0.024505    1.195172       0.256272  0.239923   0.163300    0.409366  \n",
       " 15  0.024505    1.087412       0.323328  0.239923   0.080385    0.409366  \n",
       " 16  0.043865    1.180695       0.544191  0.272636   0.153041    0.454707  \n",
       " 17 -0.011352    0.978183      -0.201310  0.174216  -0.022304    0.396632  ,\n",
       " 14:           antecedents        consequents  antecedent support  \\\n",
       " 0            (Action)        (Adventure)            0.388199   \n",
       " 1         (Adventure)           (Action)            0.199958   \n",
       " 2            (Action)            (Crime)            0.388199   \n",
       " 3             (Crime)           (Action)            0.307639   \n",
       " 4            (Action)            (Drama)            0.388199   \n",
       " 5             (Drama)           (Action)            0.502823   \n",
       " 6            (Action)           (Sci-Fi)            0.388199   \n",
       " 7            (Sci-Fi)           (Action)            0.208687   \n",
       " 8          (Thriller)           (Action)            0.449702   \n",
       " 9            (Action)         (Thriller)            0.388199   \n",
       " 10            (Crime)            (Drama)            0.307639   \n",
       " 11            (Drama)            (Crime)            0.502823   \n",
       " 12         (Thriller)            (Crime)            0.449702   \n",
       " 13            (Crime)         (Thriller)            0.307639   \n",
       " 14         (Thriller)            (Drama)            0.449702   \n",
       " 15            (Drama)         (Thriller)            0.502823   \n",
       " 16         (Thriller)          (Mystery)            0.449702   \n",
       " 17          (Mystery)         (Thriller)            0.142978   \n",
       " 18         (Thriller)           (Sci-Fi)            0.449702   \n",
       " 19           (Sci-Fi)         (Thriller)            0.208687   \n",
       " 20  (Thriller, Crime)            (Drama)            0.195199   \n",
       " 21  (Thriller, Drama)            (Crime)            0.198766   \n",
       " 22     (Crime, Drama)         (Thriller)            0.181882   \n",
       " 23         (Thriller)     (Crime, Drama)            0.449702   \n",
       " 24            (Crime)  (Thriller, Drama)            0.307639   \n",
       " 25            (Drama)  (Thriller, Crime)            0.502823   \n",
       " \n",
       "     consequent support   support  confidence      lift  representativity  \\\n",
       " 0             0.199958  0.137380    0.353890  1.769824               1.0   \n",
       " 1             0.388199  0.137380    0.687044  1.769824               1.0   \n",
       " 2             0.307639  0.122166    0.314700  1.022950               1.0   \n",
       " 3             0.388199  0.122166    0.397108  1.022950               1.0   \n",
       " 4             0.502823  0.131624    0.339063  0.674319               1.0   \n",
       " 5             0.388199  0.131624    0.261770  0.674319               1.0   \n",
       " 6             0.208687  0.131534    0.338833  1.623644               1.0   \n",
       " 7             0.388199  0.131534    0.630297  1.623644               1.0   \n",
       " 8             0.388199  0.205859    0.457768  1.179210               1.0   \n",
       " 9             0.449702  0.205859    0.530294  1.179210               1.0   \n",
       " 10            0.502823  0.181882    0.591218  1.175798               1.0   \n",
       " 11            0.307639  0.181882    0.361722  1.175798               1.0   \n",
       " 12            0.307639  0.195199    0.434063  1.410948               1.0   \n",
       " 13            0.449702  0.195199    0.634507  1.410948               1.0   \n",
       " 14            0.502823  0.198766    0.441995  0.879027               1.0   \n",
       " 15            0.449702  0.198766    0.395300  0.879027               1.0   \n",
       " 16            0.142978  0.119568    0.265884  1.859610               1.0   \n",
       " 17            0.449702  0.119568    0.836271  1.859610               1.0   \n",
       " 18            0.208687  0.105736    0.235124  1.126685               1.0   \n",
       " 19            0.449702  0.105736    0.506673  1.126685               1.0   \n",
       " 20            0.502823  0.101703    0.521020  1.036191               1.0   \n",
       " 21            0.307639  0.101703    0.511671  1.663216               1.0   \n",
       " 22            0.449702  0.101703    0.559169  1.243421               1.0   \n",
       " 23            0.181882  0.101703    0.226156  1.243421               1.0   \n",
       " 24            0.198766  0.101703    0.330591  1.663216               1.0   \n",
       " 25            0.195199  0.101703    0.202264  1.036191               1.0   \n",
       " \n",
       "     leverage  conviction  zhangs_metric   jaccard  certainty  kulczynski  \n",
       " 0   0.059756    1.238245       0.710970  0.304762   0.192405    0.520467  \n",
       " 1   0.059756    1.954909       0.543687  0.304762   0.488467    0.520467  \n",
       " 2   0.002741    1.010303       0.036671  0.212954   0.010198    0.355904  \n",
       " 3   0.002741    1.014777       0.032404  0.212954   0.014562    0.355904  \n",
       " 4  -0.063571    0.752231      -0.441165  0.173327  -0.329379    0.300416  \n",
       " 5  -0.063571    0.828740      -0.492757  0.173327  -0.206650    0.300416  \n",
       " 6   0.050523    1.196843       0.627821  0.282656   0.164468    0.484565  \n",
       " 7   0.050523    1.654843       0.485397  0.282656   0.395713    0.484565  \n",
       " 8   0.031285    1.128302       0.276169  0.325705   0.113712    0.494031  \n",
       " 9   0.031285    1.171578       0.248406  0.325705   0.146450    0.494031  \n",
       " 10  0.027194    1.216241       0.215948  0.289354   0.177794    0.476470  \n",
       " 11  0.027194    1.084732       0.300725  0.289354   0.078113    0.476470  \n",
       " 12  0.056853    1.223388       0.529271  0.347242   0.182598    0.534285  \n",
       " 13  0.056853    1.505630       0.420672  0.347242   0.335826    0.534285  \n",
       " 14 -0.027355    0.890990      -0.200055  0.263700  -0.122347    0.418648  \n",
       " 15 -0.027355    0.910035      -0.216796  0.263700  -0.098859    0.418648  \n",
       " 16  0.055271    1.167420       0.840005  0.252728   0.143410    0.551077  \n",
       " 17  0.055271    3.361034       0.539371  0.252728   0.702473    0.551077  \n",
       " 18  0.011889    1.034564       0.204326  0.191324   0.033410    0.370898  \n",
       " 19  0.011889    1.115482       0.142093  0.191324   0.103527    0.370898  \n",
       " 20  0.003552    1.037992       0.043398  0.170551   0.036602    0.361642  \n",
       " 21  0.040555    1.417815       0.497677  0.251303   0.294690    0.421131  \n",
       " 22  0.019910    1.248319       0.239289  0.191935   0.198923    0.392663  \n",
       " 23  0.019910    1.057213       0.355747  0.191935   0.054117    0.392663  \n",
       " 24  0.040555    1.196927       0.575936  0.251303   0.164527    0.421131  \n",
       " 25  0.003552    1.008856       0.070250  0.170551   0.008778    0.361642  ,\n",
       " 15:     antecedents  consequents  antecedent support  consequent support  \\\n",
       " 0      (Action)  (Adventure)            0.354508            0.304417   \n",
       " 1   (Adventure)     (Action)            0.304417            0.354508   \n",
       " 2      (Action)     (Sci-Fi)            0.354508            0.207559   \n",
       " 3      (Sci-Fi)     (Action)            0.207559            0.354508   \n",
       " 4    (Thriller)     (Action)            0.241571            0.354508   \n",
       " 5      (Action)   (Thriller)            0.354508            0.241571   \n",
       " 6      (Comedy)  (Adventure)            0.444239            0.304417   \n",
       " 7   (Adventure)     (Comedy)            0.304417            0.444239   \n",
       " 8      (Sci-Fi)  (Adventure)            0.207559            0.304417   \n",
       " 9   (Adventure)     (Sci-Fi)            0.304417            0.207559   \n",
       " 10    (Romance)     (Comedy)            0.156222            0.444239   \n",
       " 11     (Comedy)    (Romance)            0.444239            0.156222   \n",
       " \n",
       "      support  confidence      lift  representativity  leverage  conviction  \\\n",
       " 0   0.174183    0.491338  1.614031               1.0  0.066265    1.367476   \n",
       " 1   0.174183    0.572186  1.614031               1.0  0.066265    1.508817   \n",
       " 2   0.133967    0.377895  1.820662               1.0  0.060385    1.273806   \n",
       " 3   0.133967    0.645439  1.820662               1.0  0.060385    1.820539   \n",
       " 4   0.140414    0.581252  1.639602               1.0  0.054775    1.541480   \n",
       " 5   0.140414    0.396081  1.639602               1.0  0.054775    1.255845   \n",
       " 6   0.110007    0.247630  0.813458               1.0 -0.025227    0.924523   \n",
       " 7   0.110007    0.361370  0.813458               1.0 -0.025227    0.870240   \n",
       " 8   0.106485    0.513037  1.685311               1.0  0.043301    1.428410   \n",
       " 9   0.106485    0.349801  1.685311               1.0  0.043301    1.218768   \n",
       " 10  0.101528    0.649894  1.462939               1.0  0.032128    1.587409   \n",
       " 11  0.101528    0.228544  1.462939               1.0  0.032128    1.093747   \n",
       " \n",
       "     zhangs_metric   jaccard  certainty  kulczynski  \n",
       " 0        0.589369  0.359332   0.268726    0.531762  \n",
       " 1        0.546927  0.359332   0.337229    0.531762  \n",
       " 2        0.698303  0.312933   0.214951    0.511667  \n",
       " 3        0.568811  0.312933   0.450712    0.511667  \n",
       " 4        0.514347  0.308151   0.351273    0.488666  \n",
       " 5        0.604339  0.308151   0.203723    0.488666  \n",
       " 6       -0.292096  0.172250  -0.081639    0.304500  \n",
       " 7       -0.247939  0.172250  -0.149109    0.304500  \n",
       " 8        0.513146  0.262609   0.299921    0.431419  \n",
       " 9        0.584600  0.262609   0.179499    0.431419  \n",
       " 10       0.375033  0.203490   0.370043    0.439219  \n",
       " 11       0.569389  0.203490   0.085711    0.439219  ,\n",
       " 16:             antecedents          consequents  antecedent support  \\\n",
       " 0              (Action)          (Adventure)            0.643164   \n",
       " 1           (Adventure)             (Action)            0.531857   \n",
       " 2              (Action)              (Crime)            0.643164   \n",
       " 3               (Crime)             (Action)            0.153200   \n",
       " 4              (Action)              (Drama)            0.643164   \n",
       " 5               (Drama)             (Action)            0.255945   \n",
       " 6                (IMAX)             (Action)            0.187962   \n",
       " 7              (Action)               (IMAX)            0.643164   \n",
       " 8              (Action)             (Sci-Fi)            0.643164   \n",
       " 9              (Sci-Fi)             (Action)            0.429569   \n",
       " 10           (Thriller)             (Action)            0.290765   \n",
       " 11             (Action)           (Thriller)            0.643164   \n",
       " 12          (Adventure)            (Fantasy)            0.531857   \n",
       " 13            (Fantasy)          (Adventure)            0.190811   \n",
       " 14               (IMAX)          (Adventure)            0.187962   \n",
       " 15          (Adventure)               (IMAX)            0.531857   \n",
       " 16             (Sci-Fi)          (Adventure)            0.429569   \n",
       " 17          (Adventure)             (Sci-Fi)            0.531857   \n",
       " 18               (IMAX)             (Sci-Fi)            0.187962   \n",
       " 19             (Sci-Fi)               (IMAX)            0.429569   \n",
       " 20           (Thriller)             (Sci-Fi)            0.290765   \n",
       " 21             (Sci-Fi)           (Thriller)            0.429569   \n",
       " 22     (Action, Sci-Fi)          (Adventure)            0.353475   \n",
       " 23  (Action, Adventure)             (Sci-Fi)            0.387290   \n",
       " 24  (Sci-Fi, Adventure)             (Action)            0.270381   \n",
       " 25             (Action)  (Sci-Fi, Adventure)            0.643164   \n",
       " 26             (Sci-Fi)  (Action, Adventure)            0.429569   \n",
       " 27          (Adventure)     (Action, Sci-Fi)            0.531857   \n",
       " 28   (Thriller, Action)             (Sci-Fi)            0.219203   \n",
       " 29   (Thriller, Sci-Fi)             (Action)            0.140907   \n",
       " 30     (Action, Sci-Fi)           (Thriller)            0.353475   \n",
       " 31           (Thriller)     (Action, Sci-Fi)            0.290765   \n",
       " 32             (Action)   (Thriller, Sci-Fi)            0.643164   \n",
       " 33             (Sci-Fi)   (Thriller, Action)            0.429569   \n",
       " \n",
       "     consequent support   support  confidence      lift  representativity  \\\n",
       " 0             0.531857  0.387290    0.602163  1.132190               1.0   \n",
       " 1             0.643164  0.387290    0.728184  1.132190               1.0   \n",
       " 2             0.153200  0.103082    0.160274  1.046172               1.0   \n",
       " 3             0.643164  0.103082    0.672860  1.046172               1.0   \n",
       " 4             0.255945  0.114202    0.177562  0.693751               1.0   \n",
       " 5             0.643164  0.114202    0.446195  0.693751               1.0   \n",
       " 6             0.643164  0.144026    0.766251  1.191378               1.0   \n",
       " 7             0.187962  0.144026    0.223934  1.191378               1.0   \n",
       " 8             0.429569  0.353475    0.549587  1.279392               1.0   \n",
       " 9             0.643164  0.353475    0.822859  1.279392               1.0   \n",
       " 10            0.643164  0.219203    0.753883  1.172147               1.0   \n",
       " 11            0.290765  0.219203    0.340820  1.172147               1.0   \n",
       " 12            0.190811  0.164073    0.308491  1.616733               1.0   \n",
       " 13            0.531857  0.164073    0.859871  1.616733               1.0   \n",
       " 14            0.531857  0.116763    0.621203  1.167990               1.0   \n",
       " 15            0.187962  0.116763    0.219538  1.167990               1.0   \n",
       " 16            0.531857  0.270381    0.629424  1.183446               1.0   \n",
       " 17            0.429569  0.270381    0.508372  1.183446               1.0   \n",
       " 18            0.429569  0.100871    0.536659  1.249295               1.0   \n",
       " 19            0.187962  0.100871    0.234820  1.249295               1.0   \n",
       " 20            0.429569  0.140907    0.484609  1.128128               1.0   \n",
       " 21            0.290765  0.140907    0.328021  1.128128               1.0   \n",
       " 22            0.531857  0.238958    0.676027  1.271070               1.0   \n",
       " 23            0.429569  0.238958    0.617002  1.436328               1.0   \n",
       " 24            0.643164  0.238958    0.883784  1.374120               1.0   \n",
       " 25            0.270381  0.238958    0.371536  1.374120               1.0   \n",
       " 26            0.387290  0.238958    0.556275  1.436328               1.0   \n",
       " 27            0.353475  0.238958    0.449291  1.271070               1.0   \n",
       " 28            0.429569  0.119586    0.545550  1.269994               1.0   \n",
       " 29            0.643164  0.119586    0.848686  1.319549               1.0   \n",
       " 30            0.290765  0.119586    0.338316  1.163537               1.0   \n",
       " 31            0.353475  0.119586    0.411281  1.163537               1.0   \n",
       " 32            0.140907  0.119586    0.185934  1.319549               1.0   \n",
       " 33            0.219203  0.119586    0.278387  1.269994               1.0   \n",
       " \n",
       "     leverage  conviction  zhangs_metric   jaccard  certainty  kulczynski  \n",
       " 0   0.045218    1.176722       0.327198  0.491652   0.150181    0.665174  \n",
       " 1   0.045218    1.312785       0.249403  0.491652   0.238260    0.665174  \n",
       " 2   0.004549    1.008424       0.123681  0.148687   0.008353    0.416567  \n",
       " 3   0.004549    1.090774       0.052118  0.148687   0.083220    0.416567  \n",
       " 4  -0.050413    0.904694      -0.552992  0.145497  -0.105346    0.311879  \n",
       " 5  -0.050413    0.644336      -0.372368  0.145497  -0.551986    0.311879  \n",
       " 6   0.023136    1.526579       0.197818  0.209614   0.344941    0.495092  \n",
       " 7   0.023136    1.046351       0.450166  0.209614   0.044298    0.495092  \n",
       " 8   0.077191    1.266463       0.611986  0.491443   0.210399    0.686223  \n",
       " 9   0.077191    2.014416       0.382831  0.491443   0.503578    0.686223  \n",
       " 10  0.032193    1.449863       0.207075  0.306695   0.310279    0.547351  \n",
       " 11  0.032193    1.075935       0.411575  0.306695   0.070575    0.547351  \n",
       " 12  0.062589    1.170178       0.814855  0.293725   0.145429    0.584181  \n",
       " 13  0.062589    3.340797       0.471421  0.293725   0.700670    0.584181  \n",
       " 14  0.016794    1.235869       0.177120  0.193618   0.190853    0.420371  \n",
       " 15  0.016794    1.040458       0.307231  0.193618   0.038884    0.420371  \n",
       " 16  0.041912    1.263285       0.271742  0.391264   0.208413    0.568898  \n",
       " 17  0.041912    1.160289       0.331117  0.391264   0.138146    0.568898  \n",
       " 18  0.020129    1.231125       0.245738  0.195238   0.187735    0.385739  \n",
       " 19  0.020129    1.061238       0.349821  0.195238   0.057704    0.385739  \n",
       " 20  0.016004    1.106792       0.160138  0.243184   0.096488    0.406315  \n",
       " 21  0.016004    1.055441       0.199105  0.243184   0.052529    0.406315  \n",
       " 22  0.050961    1.445008       0.329857  0.369691   0.307962    0.562659  \n",
       " 23  0.072591    1.489384       0.495798  0.413494   0.328581    0.586639  \n",
       " 24  0.065059    3.070456       0.373156  0.354230   0.674315    0.627660  \n",
       " 25  0.065059    1.160956       0.762987  0.354230   0.138641    0.627660  \n",
       " 26  0.072591    1.380834       0.532545  0.413494   0.275800    0.586639  \n",
       " 27  0.050961    1.173987       0.455547  0.369691   0.148202    0.562659  \n",
       " 28  0.025423    1.255212       0.272279  0.225982   0.203322    0.411968  \n",
       " 29  0.028960    2.358255       0.281885  0.179968   0.575958    0.517310  \n",
       " 30  0.016808    1.071863       0.217395  0.227934   0.067045    0.374799  \n",
       " 31  0.016808    1.098190       0.198174  0.227934   0.089410    0.374799  \n",
       " 32  0.028960    1.055311       0.678646  0.179968   0.052412    0.517310  \n",
       " 33  0.025423    1.082016       0.372691  0.225982   0.075799    0.411968  ,\n",
       " 17:     antecedents  consequents  antecedent support  consequent support  \\\n",
       " 0      (Action)  (Adventure)            0.347039            0.233751   \n",
       " 1   (Adventure)     (Action)            0.233751            0.347039   \n",
       " 2      (Action)     (Sci-Fi)            0.347039            0.166369   \n",
       " 3      (Sci-Fi)     (Action)            0.166369            0.347039   \n",
       " 4    (Thriller)     (Action)            0.344949            0.347039   \n",
       " 5      (Action)   (Thriller)            0.347039            0.344949   \n",
       " 6       (Crime)      (Drama)            0.225050            0.440021   \n",
       " 7       (Drama)      (Crime)            0.440021            0.225050   \n",
       " 8    (Thriller)      (Crime)            0.344949            0.225050   \n",
       " 9       (Crime)   (Thriller)            0.225050            0.344949   \n",
       " 10   (Thriller)      (Drama)            0.344949            0.440021   \n",
       " 11      (Drama)   (Thriller)            0.440021            0.344949   \n",
       " \n",
       "      support  confidence      lift  representativity  leverage  conviction  \\\n",
       " 0   0.142943    0.411894  1.762104               1.0  0.061822    1.302909   \n",
       " 1   0.142943    0.611518  1.762104               1.0  0.061822    1.680803   \n",
       " 2   0.107367    0.309380  1.859602               1.0  0.049630    1.207077   \n",
       " 3   0.107367    0.645354  1.859602               1.0  0.049630    1.841163   \n",
       " 4   0.172488    0.500038  1.440871               1.0  0.052777    1.306021   \n",
       " 5   0.172488    0.497027  1.440871               1.0  0.052777    1.302358   \n",
       " 6   0.116786    0.518934  1.179338               1.0  0.017759    1.164037   \n",
       " 7   0.116786    0.265411  1.179338               1.0  0.017759    1.054943   \n",
       " 8   0.136164    0.394735  1.753986               1.0  0.058533    1.280348   \n",
       " 9   0.136164    0.605036  1.753986               1.0  0.058533    1.658508   \n",
       " 10  0.130643    0.378732  0.860714               1.0 -0.021142    0.901349   \n",
       " 11  0.130643    0.296903  0.860714               1.0 -0.021142    0.931664   \n",
       " \n",
       "     zhangs_metric   jaccard  certainty  kulczynski  \n",
       " 0        0.662362  0.326468   0.232487    0.511706  \n",
       " 1        0.564434  0.326468   0.405046    0.511706  \n",
       " 2        0.707929  0.264424   0.171552    0.477367  \n",
       " 3        0.554503  0.264424   0.456865    0.477367  \n",
       " 4        0.467101  0.332026   0.234316    0.498532  \n",
       " 5        0.468596  0.332026   0.232162    0.498532  \n",
       " 6        0.196228  0.213003   0.140921    0.392172  \n",
       " 7        0.271558  0.213003   0.052081    0.392172  \n",
       " 8        0.656239  0.313860   0.218963    0.499886  \n",
       " 9        0.554707  0.313860   0.397048    0.499886  \n",
       " 10      -0.198104  0.199661  -0.109448    0.337817  \n",
       " 11      -0.224197  0.199661  -0.073348    0.337817  ,\n",
       " 18:    antecedents  consequents  antecedent support  consequent support   support  \\\n",
       " 0     (Comedy)  (Adventure)            0.477370            0.243553  0.100353   \n",
       " 1  (Adventure)     (Comedy)            0.243553            0.477370  0.100353   \n",
       " 2  (Animation)   (Children)            0.133035            0.183579  0.116354   \n",
       " 3   (Children)  (Animation)            0.183579            0.133035  0.116354   \n",
       " 4      (Drama)     (Comedy)            0.442875            0.477370  0.131400   \n",
       " 5     (Comedy)      (Drama)            0.477370            0.442875  0.131400   \n",
       " 6    (Romance)     (Comedy)            0.314857            0.477370  0.193238   \n",
       " 7     (Comedy)    (Romance)            0.477370            0.314857  0.193238   \n",
       " 8      (Drama)    (Romance)            0.442875            0.314857  0.163194   \n",
       " 9    (Romance)      (Drama)            0.314857            0.442875  0.163194   \n",
       " \n",
       "    confidence      lift  representativity  leverage  conviction  \\\n",
       " 0    0.210220  0.863138               1.0 -0.015912    0.957794   \n",
       " 1    0.412036  0.863138               1.0 -0.015912    0.888881   \n",
       " 2    0.874613  4.764231               1.0  0.091932    6.511191   \n",
       " 3    0.633812  4.764231               1.0  0.091932    2.367538   \n",
       " 4    0.296698  0.621527               1.0 -0.080015    0.743109   \n",
       " 5    0.275259  0.621527               1.0 -0.080015    0.768723   \n",
       " 6    0.613731  1.285651               1.0  0.042934    1.353021   \n",
       " 7    0.404797  1.285651               1.0  0.042934    1.151107   \n",
       " 8    0.368487  1.170329               1.0  0.023751    1.084922   \n",
       " 9    0.518309  1.170329               1.0  0.023751    1.156603   \n",
       " \n",
       "    zhangs_metric   jaccard  certainty  kulczynski  \n",
       " 0      -0.232773  0.161711  -0.044066    0.311128  \n",
       " 1      -0.173291  0.161711  -0.125010    0.311128  \n",
       " 2       0.911344  0.581017   0.846418    0.754212  \n",
       " 3       0.967764  0.581017   0.577620    0.754212  \n",
       " 4      -0.522218  0.166573  -0.345697    0.285978  \n",
       " 5      -0.538138  0.166573  -0.300859    0.285978  \n",
       " 6       0.324289  0.322606   0.260913    0.509264  \n",
       " 7       0.425127  0.322606   0.131271    0.509264  \n",
       " 8       0.261232  0.274488   0.078275    0.443398  \n",
       " 9       0.212422  0.274488   0.135399    0.443398  ,\n",
       " 19:   antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0    (Action)    (Sci-Fi)            0.234443            0.210058  0.101450   \n",
       " 1    (Sci-Fi)    (Action)            0.210058            0.234443  0.101450   \n",
       " 2  (Thriller)    (Action)            0.418423            0.234443  0.103913   \n",
       " 3    (Action)  (Thriller)            0.234443            0.418423  0.103913   \n",
       " 4  (Thriller)     (Drama)            0.418423            0.331808  0.139713   \n",
       " 5     (Drama)  (Thriller)            0.331808            0.418423  0.139713   \n",
       " 6  (Thriller)    (Horror)            0.418423            0.441240  0.231381   \n",
       " 7    (Horror)  (Thriller)            0.441240            0.418423  0.231381   \n",
       " 8  (Thriller)   (Mystery)            0.418423            0.142157  0.108701   \n",
       " 9   (Mystery)  (Thriller)            0.142157            0.418423  0.108701   \n",
       " \n",
       "    confidence      lift  representativity  leverage  conviction  \\\n",
       " 0    0.432727  2.060037               1.0  0.052203    1.392526   \n",
       " 1    0.482960  2.060037               1.0  0.052203    1.480655   \n",
       " 2    0.248345  1.059302               1.0  0.005817    1.018496   \n",
       " 3    0.443236  1.059302               1.0  0.005817    1.044567   \n",
       " 4    0.333905  1.006318               1.0  0.000877    1.003147   \n",
       " 5    0.421066  1.006318               1.0  0.000877    1.004567   \n",
       " 6    0.552984  1.253251               1.0  0.046756    1.249979   \n",
       " 7    0.524389  1.253251               1.0  0.046756    1.222800   \n",
       " 8    0.259788  1.827474               1.0  0.049220    1.158916   \n",
       " 9    0.764656  1.827474               1.0  0.049220    2.471185   \n",
       " \n",
       "    zhangs_metric   jaccard  certainty  kulczynski  \n",
       " 0       0.672153  0.295728   0.281880    0.457844  \n",
       " 1       0.651405  0.295728   0.324623    0.457844  \n",
       " 2       0.096259  0.189294   0.018161    0.345791  \n",
       " 3       0.073126  0.189294   0.042666    0.345791  \n",
       " 4       0.010796  0.228844   0.003138    0.377486  \n",
       " 5       0.009397  0.228844   0.004546    0.377486  \n",
       " 6       0.347461  0.368276   0.199987    0.538687  \n",
       " 7       0.361650  0.368276   0.182204    0.538687  \n",
       " 8       0.778566  0.240554   0.137124    0.512222  \n",
       " 9       0.527832  0.240554   0.595336    0.512222  ,\n",
       " 20:   antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0     (Drama)    (Comedy)            0.339181            0.741735  0.181653   \n",
       " 1    (Comedy)     (Drama)            0.741735            0.339181  0.181653   \n",
       " 2   (Romance)    (Comedy)            0.281848            0.741735  0.234171   \n",
       " 3    (Comedy)   (Romance)            0.741735            0.281848  0.234171   \n",
       " 4     (Drama)   (Romance)            0.339181            0.281848  0.120151   \n",
       " 5   (Romance)     (Drama)            0.281848            0.339181  0.120151   \n",
       " \n",
       "    confidence      lift  representativity  leverage  conviction  \\\n",
       " 0    0.535565  0.722043               1.0 -0.069929    0.556083   \n",
       " 1    0.244903  0.722043               1.0 -0.069929    0.875145   \n",
       " 2    0.830841  1.120131               1.0  0.025114    1.526757   \n",
       " 3    0.315707  1.120131               1.0  0.025114    1.049480   \n",
       " 4    0.354240  1.256845               1.0  0.024554    1.112103   \n",
       " 5    0.426298  1.256845               1.0  0.024554    1.151851   \n",
       " \n",
       "    zhangs_metric   jaccard  certainty  kulczynski  \n",
       " 0      -0.368108  0.202002  -0.798292    0.390234  \n",
       " 1      -0.598484  0.202002  -0.142668    0.390234  \n",
       " 2       0.149338  0.296640   0.345017    0.573274  \n",
       " 3       0.415262  0.296640   0.047147    0.573274  \n",
       " 4       0.309248  0.239881   0.100802    0.390269  \n",
       " 5       0.284560  0.239881   0.131832    0.390269  }"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:48:48.923313Z",
     "start_time": "2025-04-27T19:48:48.672001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_user_ratings = test_set[test_set['userId'] == min(test_user_ids[test_user_ids != 0])]\n",
    "recommendations = model.recommend_for_user(test_user_ratings, top_n=10)"
   ],
   "id": "cee8cb563eeae738",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User cluster: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Data\\Studia\\magisterka\\semestr_8\\data-mining-project\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but KMeans was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:48:50.866784Z",
     "start_time": "2025-04-27T19:48:50.836448Z"
    }
   },
   "cell_type": "code",
   "source": "movies_df[movies_df['movieId'].isin(recommendations)]",
   "id": "dc5492d49c8cdf94",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       movieId                      title  \\\n",
       "454        459        Getaway, The (1994)   \n",
       "514        519           RoboCop 3 (1993)   \n",
       "11240    49530       Blood Diamond (2006)   \n",
       "15392    81132              Rubber (2010)   \n",
       "20497   105974  Assault on a Queen (1966)   \n",
       "22565   115479      Whip Hand, The (1951)   \n",
       "25042   122787        The 39 Steps (1959)   \n",
       "34202   144324    Once Upon a Time (2008)   \n",
       "49995   178865           I Witness (2003)   \n",
       "76169   251660               Erica (2019)   \n",
       "\n",
       "                                                  genres  \n",
       "454        Action|Adventure|Crime|Drama|Romance|Thriller  \n",
       "514                   Action|Crime|Drama|Sci-Fi|Thriller  \n",
       "11240          Action|Adventure|Crime|Drama|Thriller|War  \n",
       "15392  Action|Adventure|Comedy|Crime|Drama|Film-Noir|...  \n",
       "20497              Action|Adventure|Crime|Drama|Thriller  \n",
       "22565   Action|Adventure|Crime|Drama|Sci-Fi|Thriller|War  \n",
       "25042       Action|Adventure|Comedy|Crime|Drama|Thriller  \n",
       "34202  Action|Adventure|Comedy|Crime|Drama|Romance|Th...  \n",
       "49995      Action|Adventure|Crime|Drama|Mystery|Thriller  \n",
       "76169  Action|Adventure|Crime|Drama|Horror|Mystery|Th...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>459</td>\n",
       "      <td>Getaway, The (1994)</td>\n",
       "      <td>Action|Adventure|Crime|Drama|Romance|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>519</td>\n",
       "      <td>RoboCop 3 (1993)</td>\n",
       "      <td>Action|Crime|Drama|Sci-Fi|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11240</th>\n",
       "      <td>49530</td>\n",
       "      <td>Blood Diamond (2006)</td>\n",
       "      <td>Action|Adventure|Crime|Drama|Thriller|War</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15392</th>\n",
       "      <td>81132</td>\n",
       "      <td>Rubber (2010)</td>\n",
       "      <td>Action|Adventure|Comedy|Crime|Drama|Film-Noir|...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20497</th>\n",
       "      <td>105974</td>\n",
       "      <td>Assault on a Queen (1966)</td>\n",
       "      <td>Action|Adventure|Crime|Drama|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22565</th>\n",
       "      <td>115479</td>\n",
       "      <td>Whip Hand, The (1951)</td>\n",
       "      <td>Action|Adventure|Crime|Drama|Sci-Fi|Thriller|War</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25042</th>\n",
       "      <td>122787</td>\n",
       "      <td>The 39 Steps (1959)</td>\n",
       "      <td>Action|Adventure|Comedy|Crime|Drama|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34202</th>\n",
       "      <td>144324</td>\n",
       "      <td>Once Upon a Time (2008)</td>\n",
       "      <td>Action|Adventure|Comedy|Crime|Drama|Romance|Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>178865</td>\n",
       "      <td>I Witness (2003)</td>\n",
       "      <td>Action|Adventure|Crime|Drama|Mystery|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76169</th>\n",
       "      <td>251660</td>\n",
       "      <td>Erica (2019)</td>\n",
       "      <td>Action|Adventure|Crime|Drama|Horror|Mystery|Th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T20:47:32.177151100Z",
     "start_time": "2025-04-22T20:13:18.660629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = [res if not np.isnan(res) else 0 for res in results ]\n",
    "print(results)\n",
    "print(f\"Mean NDCG: {np.mean(results)}\")\n",
    "print(f\"Standard Deviation NDCG: {np.std(results)}\")\n"
   ],
   "id": "401a1a3822dfbb97",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(0.6860571089196594), np.float64(-0.01297498240269205), np.float64(-0.12349650381112376), np.float64(0.07237468644557458), np.float64(0.5148232810904031), np.float64(0.37877700953928506), np.float64(0.10699444587365947), np.float64(0.4671457034214359), np.float64(0.5415386082446842), np.float64(0.20412414523193154), np.float64(-0.18422647458873528), np.float64(0.2195663143362036), np.float64(0.42870375986597026), np.float64(0.49760066534670505), np.float64(0.1859910555043518), np.float64(0.345643126531901), np.float64(0.2472066162365221), np.float64(-0.24253732318886512), np.float64(0.16414505016140593), np.float64(0.18019071590012917), np.float64(0.5912866770718964), np.float64(-0.01946247360403807), np.float64(0.34151450937027694), np.float64(0.32741472132289046), np.float64(-0.10815649964574403), np.float64(0.5127504275741923), np.float64(0.09380352942419311), np.float64(-0.45905523340560367), np.float64(0.44263134656228503), np.float64(-0.019069251784911846), np.float64(0.06919990614769093), np.float64(0.17755474535281088), np.float64(-0.0957300944019763), np.float64(0.5036957760601789), np.float64(0.29213563322602576), np.float64(0.25408251972394386), np.float64(0.49596793915695975), np.float64(0.24063082784843445), np.float64(0.16445727667559737), np.float64(0.4355700338124751), np.float64(0.31686799642267366), np.float64(-0.23725837087952056), np.float64(-0.23620076410453292), np.float64(-0.07815782178862689), np.float64(0.22247460415730486), np.float64(0.28705891402628697), np.float64(0.323047869190461), np.float64(0.29394016056998745), np.float64(-0.13297799396284804), np.float64(-0.034068502133400735), np.float64(0.3166841199613375), np.float64(0.471124231979254), np.float64(0.5433030434895331), np.float64(0.2948110924760355), np.float64(0.2576198956192664), np.float64(0.29308876996141914), np.float64(0.4735098832024755), np.float64(0.42628348220559753), np.float64(0.3494296970386772), np.float64(0.5254270830780441), np.float64(0.0778498944161523), np.float64(-0.036305253798137926), np.float64(0.2609148651219738), np.float64(0.3483373259769498), np.float64(0.7378603713872524), np.float64(0.10545410080165932), np.float64(0.5911468053322673), np.float64(0.382002252114883), np.float64(0.25625934117479326), np.float64(0.2187431857868904), np.float64(0.06030226891555272), np.float64(0.5916163477719995), np.float64(-0.2777830701439635), np.float64(-0.07078383972965656), np.float64(-0.04798010706723878), np.float64(-0.06030226891555272), np.float64(0.7099982954692763), np.float64(-0.09500523598840124), np.float64(0.5860604634425701), np.float64(0.34036270975761784), np.float64(-0.06854301009605542), np.float64(0.43427157442838255), np.float64(-0.028813538628606524), np.float64(0.3618136134933163), np.float64(0.5106882308569509), np.float64(0.08341955511876985), np.float64(0.26028289120084586), np.float64(0.1179617286478382), np.float64(-0.16439903891637445), np.float64(-0.5393148141619877), np.float64(-0.04404694129782468), np.float64(0.6235637203686687), np.float64(0.35510949070562176), np.float64(0.09760860118037878), np.float64(-0.15622591764872515), np.float64(-0.002200389334095557), np.float64(0.12535532601135183), np.float64(0.3650314128926685), np.float64(0.36867220117496236), np.float64(-0.013544151741672042), np.float64(-0.14674036879715754), np.float64(0.4584064269201681), np.float64(0.5131620992985864), np.float64(0.4281744192888376), np.float64(0.7246315678266502), np.float64(-0.0031061293466901644), np.float64(0.32161210088294784), np.float64(-0.08594847818823075), np.float64(0.14417234657743258), np.float64(-0.7941985987087266), np.float64(-0.6227991553292184), np.float64(0.4464294687205575), np.float64(0.50482307365789), np.float64(0.4561440882654101), np.float64(0.5338310879203045), np.float64(0.6729549061404444), np.float64(0.343069251229448), np.float64(-0.23187354568389557), np.float64(-0.29295608483431346), np.float64(0.5049271045438433), np.float64(0.12455594886300343), np.float64(-0.21701135543121325), np.float64(0.17337673646155619), np.float64(0.1803909095748864), np.float64(-0.15666666666666668), np.float64(0.2977118327157717), np.float64(0.738988427584557), np.float64(0.2778745619360801), np.float64(0.10484470223637496), np.float64(0.28509213783133797), np.float64(0.3677458160699931), np.float64(0.26267321197250465), np.float64(0.41978507857607567), np.float64(0.06576101679733735), np.float64(0.5085133809309825), np.float64(0.22973414586817034), np.float64(0.6563139182320222), np.float64(0.01538527086573974), np.float64(0.33077848869696413), np.float64(0.038012007355450836), np.float64(0.5325863028766383), np.float64(0.0382546027838003), np.float64(0.49402722713968644), np.float64(0.7454128010052143), np.float64(0.17338388510976466), np.float64(0.06099942813304188), np.float64(-0.49559253807314524), np.float64(0.18784784055571507), np.float64(-0.08996312631323775), np.float64(0.0382546027838003), np.float64(-0.2475059943583897), np.float64(-0.32500634784222254), np.float64(-0.14598807929443774), np.float64(0.1956970327337142), np.float64(0.6610673952102774), np.float64(0.23222439707067227), np.float64(0.19751318678165308), np.float64(0.4898988539357031), np.float64(0.023597772852905617), np.float64(-0.12974982402692048), np.float64(0.2755748151671524), np.float64(0.3959116009922194), np.float64(0.14322297480788657), np.float64(-0.13965865839977587), np.float64(-0.030938249294798714), np.float64(-0.03232540919176179), np.float64(0.2637073406906887), np.float64(-0.1874873733122184), np.float64(0.4937619252644684), np.float64(0.8186349431603627), np.float64(0.19237932910979264), np.float64(-0.33235971740831943), np.float64(0.41308311050281377), np.float64(0.09271040345873598), np.float64(0.4797833843364064), np.float64(0.33159425678262544), np.float64(-0.4734215945013507), np.float64(-0.6817986171707132), np.float64(0.18382292413643048), np.float64(-0.07765323366725412), np.float64(-0.19728740087740032), np.float64(0.4540219081634881), np.float64(0.38144809018088616), np.float64(0.31878835653166915), np.float64(0.1300097404555384), np.float64(0.6290219623352464), np.float64(0.4151994368861456), np.float64(-0.5568588179415552), np.float64(-0.04986383946711772), np.float64(-0.04385290096535146), np.float64(-0.11790213696500118), np.float64(0.24065087582782083), np.float64(0.181476286689944), np.float64(0.13153857011803996), np.float64(0.5247813488251173), np.float64(0.3366361941239161), np.float64(0.48322241114778364), np.float64(-0.21635274585338146), np.float64(0.5195120326989651), np.float64(0.38931135102115416), np.float64(0.7745360140854262), np.float64(0.4911603518592388), np.float64(0.05076298340185718), np.float64(-0.2937012370873958), np.float64(0.34947989950106), np.float64(0.42599897728156577), np.float64(0.557148938327407), np.float64(-0.4861588361460226), np.float64(0.19169651618850775), np.float64(-0.2426843742837029), np.float64(0.1598514503776719), 0, np.float64(0.4057513356003446), np.float64(0.13576671860539952), np.float64(0.39309543182219087), np.float64(0.09016696346674323), np.float64(0.566381764013006), np.float64(0.2566011610343099), np.float64(0.7542095746883586), np.float64(0.2823133602241932), np.float64(0.05556508399324771), np.float64(0.20442821831160987), np.float64(-0.224109730644077), np.float64(-0.1803209153582371), np.float64(0.2090209601328732), np.float64(0.6486717897814435), np.float64(0.19773406865733265), np.float64(0.11427797704737139), np.float64(0.5188692980929639), np.float64(0.5321747178451723), np.float64(0.14519079579805494), np.float64(0.14992587245787106), np.float64(-0.2958081738859997), np.float64(0.49735200604648133), np.float64(0.1704496542926783), np.float64(-0.5000432881694244), np.float64(0.23619507949682322), np.float64(0.2664880308962519), np.float64(0.3007835577731087), np.float64(0.1726454381937326), np.float64(-0.06251954041004443), np.float64(-0.06249999999999999), np.float64(0.09016696346674324), np.float64(0.7615487029831055), np.float64(0.2251417299272887), np.float64(0.22110831935702668), np.float64(0.2069102204422663), np.float64(0.3874407723454903), np.float64(0.050015632328035534), np.float64(0.21709014165170284), np.float64(0.2119583104847831), np.float64(0.099556303894016), np.float64(0.5540459091245176), np.float64(0.33186629607384577), np.float64(-0.016703827619526525), np.float64(0.21960261528947078), np.float64(0.6311687442672026), np.float64(-0.10815649964574403), np.float64(0.2006325743194479), np.float64(0.35176560709218985), np.float64(0.6286185570937121), np.float64(0.46303244639028446), np.float64(-0.4624794228246519), np.float64(0.40004734568283135), np.float64(0.33722643761127125), np.float64(0.45376098965704775), np.float64(0.4369314487526515), np.float64(0.6060432152628561), np.float64(-0.06956532259414018), np.float64(0.4506927775986437), np.float64(-0.16450322423053299), np.float64(0.18938850476964253), np.float64(-0.17958808179432403), np.float64(0.6943320267318509), np.float64(0.2400288055733796), np.float64(-0.14428643513521952), np.float64(0.4794310133071944), np.float64(-0.24317859368630657), np.float64(0.5345420705058799), np.float64(-0.010874134181534819), np.float64(0.18434810487447148), np.float64(-0.10980735214101901), np.float64(0.4523838666339657), np.float64(0.1563263498701806), np.float64(0.38452864167092315), np.float64(-0.11182065648084594), np.float64(0.4061711508555291), np.float64(0.520810817483224), np.float64(0.15312799081027792), np.float64(-0.325101610132231), np.float64(0.557509441213279), np.float64(0.31234752377721214), np.float64(0.39527463057643025), np.float64(0.4622196005163612), np.float64(-0.1455710489137156), np.float64(-0.006293790933744675), np.float64(-0.389565806527185), np.float64(0.3428632472163112), np.float64(0.4909654340155108), np.float64(0.250804045264487), np.float64(0.25254705328768695), np.float64(0.3024291894545669), np.float64(0.354005510612021), np.float64(-0.004248205090805263), np.float64(0.343388726440763), np.float64(0.23398742031909198), np.float64(0.5577682118226356), np.float64(-0.018881372801234024), np.float64(0.29009740444381893), np.float64(0.11713032141645457), np.float64(-0.17443400772261658), np.float64(0.0820233426908302), np.float64(0.3147526872812686), np.float64(0.5662208585049306), np.float64(0.01912730139190015), np.float64(0.11584228044680323), np.float64(0.10854902026978494), np.float64(-0.03296171265436078), np.float64(0.3813850356982369), np.float64(0.2983328583183717), np.float64(0.4149103958696451), np.float64(0.3492293202898956), np.float64(-0.06867036569264849), np.float64(0.331329117001076), np.float64(-0.015578213909866991), np.float64(0.7431230970799615), np.float64(0.41477603283737324), np.float64(0.2876404837611323), np.float64(0.40562135715614567), np.float64(0.2167068243379528), np.float64(-0.26520733064842883), np.float64(0.13563254229811467), np.float64(0.3480424005599184), np.float64(0.13304202574407792), np.float64(-0.05939356338954221), np.float64(0.5460398421383316), np.float64(0.1874873733122184), np.float64(0.5697932652907494), np.float64(0.053767838039199024), np.float64(0.6876740673426784), np.float64(0.559134669777656), np.float64(0.30949223029508643), np.float64(0.08819682643992686), np.float64(0.7101303430419309), np.float64(-0.43606507337305833), np.float64(-0.18993429409939658), 0, np.float64(0.119884666486075), np.float64(0.747118810047004), np.float64(0.06843502260811289), np.float64(-0.04404694129782468), np.float64(0.1211395754820343), np.float64(0.13525044520011487), np.float64(-0.6929523265198467), np.float64(0.19018949237734636), np.float64(0.029455833260230484), np.float64(0.30688485607848515), np.float64(-0.010741723110591494), np.float64(0.3553345272593507), np.float64(0.1341488353302029), np.float64(0.352782145074511), np.float64(0.23304623432658841), np.float64(0.4025645210643298), np.float64(0.161049484103565), np.float64(0.47479231262930405), np.float64(0.055048188256318034), np.float64(-0.12316169428402525), np.float64(-0.31333978072025614), np.float64(-0.07585846103890043), np.float64(0.07820452790170071), np.float64(0.5116234228489994), np.float64(0.23063128468116423), np.float64(-0.3882178117902762), np.float64(-0.03739787960033829), np.float64(0.3730778961080611), np.float64(0.41666666666666663), np.float64(0.8029853781438303), np.float64(0.44891689081578134), np.float64(0.4879392458595668), np.float64(-0.1740776559556978), np.float64(0.25231767870474053), np.float64(0.5442736889077289), np.float64(0.5128762019474578), np.float64(0.5638447757725207), np.float64(-0.24438392398876704), np.float64(0.6055365068554401), np.float64(0.46692614217554906), np.float64(0.4640184600995216), np.float64(-0.21453772938759552), np.float64(0.22915598712780136), np.float64(0.16491594566332277), np.float64(0.026247619494999107), np.float64(-0.5521878847579915), np.float64(0.384166516027929), np.float64(0.5223846475008079), np.float64(-0.031468954668723376), np.float64(0.17676260445166636), np.float64(0.26740161484246083), np.float64(0.10312749764932903), np.float64(0.34276063541848123), np.float64(0.13363739956755885), np.float64(0.7459920006545984), np.float64(0.25993585333512836), np.float64(-0.038799179683158526), np.float64(-0.5349722293682974), np.float64(0.14119691624892292), np.float64(0.52745709050175), np.float64(0.39488849087132755), np.float64(0.2723780854199167), np.float64(0.4314788022165075), np.float64(-0.4128614119223852), np.float64(-0.04541300066320286), np.float64(0.08589603667957882), np.float64(0.17978662999019787), np.float64(0.4015791849349912), np.float64(0.3829708431025352), np.float64(0.4781825347975037), np.float64(0.4966913037369668), np.float64(-0.07808688094430305), np.float64(0.6565009360958634), np.float64(0.15147243132741753), np.float64(-0.6388067453025184), np.float64(0.48589461279111745), np.float64(-0.3277077119898274), np.float64(0.09587062360592129), np.float64(0.3933535884797413), np.float64(0.35363224129282156), np.float64(0.4277550977806634), np.float64(-0.3488454123148558), np.float64(0.1818905871137), np.float64(0.5107926673226079), np.float64(-0.5222329678670935), np.float64(0.36231816996426164), np.float64(0.8002922379634526), np.float64(-0.33657684430963986), np.float64(0.40859817095854695), np.float64(0.25640853367375127), np.float64(0.6890549673048442), np.float64(0.2573070083610708), np.float64(-0.5201564866102993), np.float64(0.6150675007909986), np.float64(0.4047542796516864), np.float64(0.21812040435721533), np.float64(0.16727466382178907), np.float64(-0.3060368222704024), np.float64(0.45700410850912343), np.float64(0.7888106377466153), np.float64(0.41895461765208397), np.float64(-0.03728333758674101), np.float64(-0.07896181016336831), np.float64(0.49729396774049334), np.float64(0.4636654369430009), np.float64(0.3961674580193108), np.float64(-0.036295416072102724), np.float64(0.28544961285922504), np.float64(0.09869275424396533), np.float64(0.48308771188443345), np.float64(-0.17852147965773474), np.float64(0.43318121770736173), np.float64(0.41316712200642336), np.float64(0.2594166117196963), np.float64(-0.027417204038422165), np.float64(0.5986906887760554), np.float64(0.0), np.float64(-0.01258758186748935), np.float64(0.3497530870630646), np.float64(0.574172534596893), np.float64(0.7128473049989762), np.float64(0.21300321680756462), np.float64(0.6260684646969837), np.float64(0.0015509585583836042), np.float64(0.27492611736811756), np.float64(0.017809973237147603), np.float64(0.1704496542926783), np.float64(0.31122611294981406), np.float64(0.41415009146461523), np.float64(0.517661913037023), np.float64(-0.04825171993640425), np.float64(0.1211265101729492), np.float64(0.4864327847308754), np.float64(0.17823503833072715), np.float64(0.5124335523821733), np.float64(-0.09100079877627214), np.float64(0.38681248501059107), np.float64(0.0260405408741612), np.float64(0.07806806017366867), np.float64(0.16829045820152227), np.float64(0.26885687497432265), np.float64(0.4993502271458874), np.float64(0.04405653653621271), np.float64(0.46709936649691375), np.float64(0.772146878122115), np.float64(0.4335463555824993), np.float64(-0.3156475079494042), np.float64(0.4514782933196294), np.float64(0.07545074067997101), np.float64(-0.09314579769596107), np.float64(0.26343739793893073), np.float64(0.13483997249264842), np.float64(0.4477898780553109), np.float64(-0.18808706005538697), np.float64(0.05725616704217664), np.float64(-0.08078767603922848), np.float64(0.5797655118929443), np.float64(0.5018239914653673), np.float64(-0.13693063937629155), np.float64(0.2622905525080382), np.float64(-0.17407765595569782), np.float64(-0.23460813241593478), np.float64(0.23352314803468627), np.float64(0.12812286773437795), np.float64(-0.33436333450068045), np.float64(0.3084435454322494), np.float64(0.18069766994540395), np.float64(0.15238922883268421), np.float64(0.78408507412694), np.float64(0.41785544701867244), np.float64(0.1787638714593372), np.float64(0.7214321756760975), np.float64(0.26520733064842883), np.float64(0.18936863780054028), np.float64(0.03564162178201257), np.float64(-0.2713189758521599), np.float64(0.5003171406338437), np.float64(0.21548188044157673), np.float64(0.4137740262436134), np.float64(0.4494665749754947), np.float64(0.3728080383302709), 0, np.float64(-0.4947182194895883), np.float64(0.1957014549286306), np.float64(0.30321977098871544), np.float64(0.13257195333874977), np.float64(0.6230853024407228), np.float64(-0.11599633400095509), np.float64(0.3535533905932738), np.float64(0.2696201688690832), np.float64(0.3876211505422754), np.float64(0.3956886776903678), np.float64(-0.33791496280964733), np.float64(0.016979258355400647), np.float64(0.03278071906869416), np.float64(0.04025250109183125), np.float64(0.3787905855515567), np.float64(0.8697814092129027), np.float64(0.1362392344646596), np.float64(-0.5987360424133898), np.float64(0.28252184712745376), np.float64(0.3158168135806891), np.float64(0.8734800968967735), np.float64(0.14428643513521952), np.float64(0.5667838859899658), np.float64(-0.040615608012903606), np.float64(-0.03132492769578745), np.float64(0.30675920399552287), np.float64(0.5049040411574732), np.float64(0.4585670197642731), np.float64(0.12892113063809887), np.float64(0.13025455347667705), np.float64(-0.21864249413386816), np.float64(-0.544949260913066), np.float64(-0.0130202704370806), np.float64(0.7808688094430303), np.float64(0.26919095102908275), np.float64(-0.3937480273916), np.float64(0.24017019397312328), np.float64(-0.0031061293466901644), np.float64(0.1837225310227988), np.float64(0.11693691643036609), np.float64(-0.18943380760602058), np.float64(0.6859245370246428), np.float64(-0.14113362174546457), np.float64(0.3235020708515275), np.float64(-0.027500666039821077), np.float64(-0.3589024597432513), np.float64(-0.13528407622809346), np.float64(0.5144795876300068), np.float64(-0.16993235521110622), np.float64(-0.2901294265928297), np.float64(0.2133049964872337), np.float64(0.5338310879203045), np.float64(0.13102825088537348), np.float64(0.19069251784911845), 0, np.float64(0.2912876325017676), np.float64(0.9185342481327662), np.float64(0.3685209512772383), np.float64(0.299733688362958), np.float64(0.08849449235023643), np.float64(0.0920654235162723), np.float64(0.48308771188443345), np.float64(-0.21701135543121325), np.float64(0.081199794294115), np.float64(0.03058232680906557), np.float64(0.3411211461689767), np.float64(0.03942568019883491), np.float64(-0.07976060548818034), np.float64(0.055483358571765264), np.float64(0.3008517334410915), np.float64(0.5187236008972574), np.float64(0.3790738492289503), np.float64(0.08788513536778761), np.float64(-0.2063235173129512), np.float64(-0.026304406718934942), np.float64(-0.04494665749754947), np.float64(0.32741129757509446), np.float64(-0.18028184977888131), np.float64(0.5436000574824175), np.float64(0.08505172717997146), np.float64(0.6525394596019061), np.float64(0.3654108983929805), np.float64(-0.2004561009344103), np.float64(-0.3447856731196243), np.float64(-0.023951380900976916), np.float64(0.5314160934853777), np.float64(-0.11366028456361237), np.float64(0.005276622421669611), np.float64(0.6434557317684291), np.float64(0.28494493423249445), np.float64(0.1753507287584581), np.float64(0.3473960175536749), np.float64(0.3626290436518547), np.float64(0.31016430103644177), np.float64(0.2412844888501289), np.float64(-0.22000532831856862), np.float64(0.47766635743840635), np.float64(0.25855725062711327), np.float64(0.471124231979254), np.float64(0.041967278279439954), np.float64(0.1421363535252001), np.float64(-0.02764874207565295), np.float64(-0.28426762180748055), np.float64(0.7655329054270494), np.float64(0.07947049415071886), np.float64(-0.13217411292886636), np.float64(0.014126448280440337), np.float64(-0.15079130771599314), np.float64(0.8257228238447704), np.float64(-0.7236272269866326), np.float64(-0.07063046178186344), np.float64(0.717845449393623), np.float64(0.639201056618704), np.float64(-0.24078203063740672), np.float64(-0.1371255585345737), np.float64(0.5622290114063266), np.float64(0.8218053479440763), np.float64(0.3281953757567687), np.float64(0.5128225940683708), np.float64(0.486884254270229), np.float64(-0.5921588471609647), np.float64(0.1666769157743375), np.float64(-0.06918852880998637), np.float64(0.2177967804850889), np.float64(0.14540507905776087), np.float64(0.11886310604759964), np.float64(0.0), np.float64(-0.14156588499882666), np.float64(0.40068240962281354), np.float64(0.3233171047350098), np.float64(0.5062596541607013), np.float64(0.1991772128492551), np.float64(-0.041382044088453265), np.float64(0.3918514551371243), np.float64(-0.31452139837349885), np.float64(-0.33734954246999327), np.float64(0.2293511458027395), np.float64(0.42216907657704944), np.float64(0.022851430193530572), np.float64(0.14326068523930902), np.float64(-0.10170267618619652), np.float64(0.1191696950055276), np.float64(0.48007434889960576), np.float64(-0.2477362023003258), np.float64(-0.05556508399324771), np.float64(0.17809975779034737), np.float64(0.390216847389566), np.float64(0.10990639125199012), np.float64(-0.01403557667909589), np.float64(-0.4130370602655465), np.float64(-0.485479387502946), np.float64(0.1007006549399148), np.float64(-0.6499244538676238), np.float64(0.1912730139190015), np.float64(0.47374097438847546), np.float64(0.22438727760202973), np.float64(0.5663154045476679), np.float64(0.24618298195866545), np.float64(0.2922006915120909), np.float64(-0.012712834523274565), np.float64(0.21608643775151884), np.float64(0.18145751367274018), np.float64(0.5537329763155102), np.float64(0.058025885318565944), np.float64(0.19581511249698932), np.float64(0.10803076808138597), np.float64(0.06030226891555272), np.float64(0.3554824185485994), np.float64(0.0885975874788177), np.float64(0.28982486945509367), np.float64(0.1510509824098722), np.float64(0.2827723722285506), np.float64(0.4193851571072714), np.float64(0.17407765595569782), np.float64(-0.026362041208835618), np.float64(0.14907119849998599), np.float64(0.5047929357981269), np.float64(-0.10753942876134207), np.float64(0.6645312119093666), np.float64(0.5608575979751068), np.float64(-0.04145206351439161), np.float64(0.16087236302194674), np.float64(-0.29675605647269965), np.float64(0.23205052700007597), np.float64(0.14548555869288923), np.float64(0.11328823680740414), np.float64(0.7182751505650129), np.float64(0.611528698054681), np.float64(0.14213381090374028), np.float64(0.056816551430892766), np.float64(0.26667876121581074), np.float64(0.20472177166684513), np.float64(0.022734943086157085), np.float64(0.7451063099170143), np.float64(0.32961712654360775), np.float64(0.21334770066562783), np.float64(0.3814155941723614), np.float64(0.02758489492323652), np.float64(0.1348399724926484), np.float64(0.025761989561926636), np.float64(-0.5085133809309825), np.float64(0.24738513830453138), np.float64(0.583669478178447), np.float64(0.342205840322677), np.float64(0.3064292183057618), np.float64(0.23230635664462743), np.float64(0.33629258092164915), np.float64(0.21170043261819688), np.float64(-0.057641701563312504), np.float64(-0.7291457433631238), np.float64(0.6414333303719606), np.float64(0.6038596398555418), np.float64(0.3277732311314077), np.float64(0.4917107860304124), np.float64(0.46064539986458847), np.float64(0.24941649696252097), np.float64(0.28857287027043904), np.float64(0.4772294584719452), np.float64(0.3506334920077187), np.float64(0.318511028635303), np.float64(0.49720948376582924), np.float64(0.35957325998039574), np.float64(-0.37562563765809204), np.float64(0.3374487155269168), np.float64(-0.12825608379126602), np.float64(0.42622381204240867), np.float64(0.7304278883838325), np.float64(0.20181419177474927), np.float64(0.5556839618869357), np.float64(0.49240541473778743), np.float64(0.18598740332788405), np.float64(0.31096829182323454), np.float64(-0.11334529009772461), np.float64(-0.13525044520011484), np.float64(0.058185736545171236), np.float64(0.29966105513976904), np.float64(0.6250541102117806), np.float64(0.48185441851989574), np.float64(0.3167667279620903), np.float64(0.13226802601388093), np.float64(-0.30321977098871544), np.float64(0.4261241357316957), np.float64(0.3598381590623208), np.float64(0.17407765595569782), np.float64(0.013708004012012201), np.float64(0.02254174086668581), np.float64(-0.29217435489538873), np.float64(-0.10136535294786891), np.float64(0.6065462374963008), np.float64(0.37859596452003774), np.float64(0.30492337569071715), np.float64(0.022370002552044604), np.float64(-0.39315794629093703), np.float64(0.47364309822867545), np.float64(0.2529152772418504), np.float64(0.6335564128181457), np.float64(-0.1880851303536848), np.float64(0.6099802088531351), np.float64(0.8235934927014349), np.float64(-0.5843065474681431), np.float64(-0.2377698636841059), np.float64(0.41800674210747835), np.float64(0.4470778605226019), np.float64(0.50240879656414), np.float64(0.40529261415681817), np.float64(0.5371291452680611), np.float64(-0.026781832691901142), np.float64(-0.1317729766079425), np.float64(0.11009637651263607), np.float64(0.4393503064839869), np.float64(0.0), np.float64(0.16957435990759992), np.float64(0.37310438707059995), np.float64(0.40130338439567786), np.float64(-0.0842835582994015), np.float64(-0.11007930809834743), np.float64(0.5263613559678152), np.float64(0.2556896087358144), np.float64(0.39551873723098685), np.float64(0.12483755678647185), np.float64(0.45863035816856906), np.float64(0.47069426905638256), np.float64(0.3428138963364343), np.float64(-0.15255401427929477), np.float64(0.14816043185068212), np.float64(0.26103889769385047), np.float64(0.6054425753242478), np.float64(0.07861672269560334), np.float64(0.08811307307242543), np.float64(0.11572623019357381), np.float64(0.03463439760654252), np.float64(-0.0251751637349787), np.float64(0.6657713824897639), np.float64(0.6725365263078101), np.float64(0.4945502459805859), np.float64(-0.4380700968236863), np.float64(0.34369443239948216), np.float64(-0.2099125395300469), np.float64(-0.28191418399076984), np.float64(0.5103493078392206), np.float64(0.0075139136222286025), np.float64(0.11808167077151335), np.float64(0.4210206685678798), np.float64(-0.13936627801370885), np.float64(0.5273209527017643), np.float64(0.4504155258853252), np.float64(0.27048137303735564), np.float64(0.4370240855466457), np.float64(0.5002474047395803), np.float64(0.17287871223515455), np.float64(0.34429142505420274), np.float64(-0.6478835438717), np.float64(-0.6625891564490793), 0, np.float64(0.13846340054238282), np.float64(0.34405696572805156), np.float64(0.5741234462860217), np.float64(0.22500439466000025), np.float64(0.09288705013068531), np.float64(0.4862113893683642), np.float64(0.6349812749303677), np.float64(0.6293790933744674), np.float64(0.23028309323591914), np.float64(-0.10585122480499264), np.float64(0.4751503792598087), np.float64(0.18058514663619765), np.float64(-0.1545985534856562), np.float64(-0.09044312632922691), np.float64(0.7666565318069348), np.float64(0.1362373152282665), np.float64(-0.008893721359049788), np.float64(0.5756398451820646), np.float64(0.6359630718220475), np.float64(-0.1919955567827525), np.float64(0.018274372925651903), np.float64(0.1539942509886093), np.float64(-0.28650238630714925), np.float64(-0.06260879033472617), np.float64(-0.3423265984407289), np.float64(0.4817761205077073), np.float64(-0.004519077332078759), np.float64(0.7743615693643384), np.float64(0.39378165675898114), np.float64(0.34413595868208974), np.float64(0.4494665749754947), np.float64(-0.22933792747768256), np.float64(-0.006935419821470658), np.float64(0.3834442083922587), np.float64(-0.26084014431450725), np.float64(-0.2288688541085317), np.float64(-0.5698028822981897), np.float64(-0.4737252203710337), np.float64(0.31143133647898363), np.float64(0.4523118824008926), np.float64(0.07895420339517227), np.float64(0.4672706462058845), np.float64(0.4048449843158238), np.float64(0.6584994214983149), np.float64(0.48656184010095177), np.float64(0.29634711463065444), np.float64(0.3376055182142399), np.float64(0.10736388502431087), np.float64(0.5278044196417837), np.float64(0.20135744593291283), np.float64(0.4808292144163532), np.float64(0.5797655118929443), np.float64(-0.4058611019585274), np.float64(0.0571791430348098), np.float64(0.4198911048651824), np.float64(0.0902431721712947), np.float64(-0.1205908351056359), np.float64(0.5776051262254929), np.float64(0.20769510081357426), np.float64(0.029172933710605944), np.float64(-0.18145751367274018), np.float64(0.12422067255383758), np.float64(-0.08840244354947627), np.float64(0.18812468899362855), np.float64(0.26424246385052935), np.float64(-0.14130462377314468), np.float64(-0.2202826826810636), np.float64(0.4917107860304124), np.float64(0.6525394596019061), np.float64(0.683349073208448), np.float64(-0.09782797401561578), np.float64(0.5157555505070089), np.float64(-0.388955587952734), np.float64(-0.02322675390980627), np.float64(-0.13142880109298877), np.float64(0.4153902016271485), np.float64(0.3906951575475158), np.float64(0.32194255046156384), np.float64(0.3653485736401024), np.float64(0.13767902008851546), np.float64(-0.24651724278058584), np.float64(0.7304358872384719), np.float64(-0.040637701266528874), np.float64(0.2329521413309568), np.float64(0.07680725537733786), np.float64(0.6165724743788163), np.float64(0.03437583254977635), np.float64(0.7188828613865151), np.float64(0.49310442097109447), np.float64(0.31981609254626914), np.float64(-0.23586408826243163), np.float64(-0.4824181513244218), np.float64(-0.6537104851338884), np.float64(0.29565262102509576), np.float64(0.040090304893565805), np.float64(-0.26939752495070407), np.float64(0.21170043261819688), 0, np.float64(-0.01699962294310051), np.float64(0.7034148205425312), np.float64(0.6398687904834224), np.float64(0.05818960937838717), np.float64(0.3171537501223347), np.float64(0.5265973011232242), np.float64(0.21767601657541885), np.float64(0.43162815156736684), np.float64(0.36520511894957564), np.float64(0.7656770417228386), np.float64(0.3583232226089713), np.float64(0.21959616814557623), np.float64(0.12821215295120647), np.float64(0.08282943498909671), np.float64(-0.1306188399008633), np.float64(0.009416865531846006), np.float64(-0.07602401471090167), np.float64(0.4181234051952916), np.float64(0.2946841018048533), np.float64(0.7776151894036842), np.float64(0.384571464806011), np.float64(0.11914920172391069), np.float64(0.5839664468306304), np.float64(0.5259963433566813), np.float64(0.2974188403044618), np.float64(0.14022519552416016), np.float64(0.5782704426945936), np.float64(0.0893503034748262), np.float64(0.40862447879106345), np.float64(-0.08642633970683909), np.float64(0.29138795786987703), np.float64(-0.16895782896178466), np.float64(0.1715320096175801), np.float64(-0.3439179697143973), np.float64(0.2505627314425386), np.float64(0.31339158526400435), np.float64(-0.7503664793768668), np.float64(0.3618136134933163), np.float64(0.109047303602929), np.float64(0.23168585599927483), np.float64(0.10882988117292028), np.float64(0.30474378531085083), np.float64(0.06062861263154311), np.float64(-0.5222329678670935), np.float64(-0.26598713925518697), np.float64(0.3011195168039364), np.float64(0.23757518962990434), np.float64(0.13534750215254554), np.float64(0.48931722894504914), np.float64(0.38171094022448676), np.float64(0.5876836798544176), np.float64(-0.2075997184430728), np.float64(0.4155322576134834), np.float64(0.14959151840135315), np.float64(0.590869970085166), np.float64(-0.10170267618619652), np.float64(0.4764741838522266), np.float64(0.039634146341463415), np.float64(0.244116855803904)]\n",
      "Mean NDCG: 0.19628216515652464\n",
      "Standard Deviation NDCG: 0.30915010258969305\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T21:28:20.283105Z",
     "start_time": "2025-04-09T21:28:19.738239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sns.histplot(results)\n",
    "plt.show()"
   ],
   "id": "2a77f5895bd7f27f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
